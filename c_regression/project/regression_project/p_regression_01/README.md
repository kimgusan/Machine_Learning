# â‘  Regression01

## ì£¼ì œ: ğŸ˜Š ì§ì—…ì  ì‚¶ê³¼ ê°œì¸ ìƒí™œ ì „ë°˜ì ì¸ ë§Œì¡± ì ìˆ˜

    (1) ë°ì´í„° ì›ë³¸: ex) https://kaggle.com)

### ëª©ì°¨

1. **ê°€ì„¤ ì„¤ì •**
2. **ë°ì´í„° ë¶„ì„**
3. **ë°ì´í„° ì „ì²˜ë¦¬**
4. **ë°ì´í„° í›ˆë ¨**
    <details>
        <summary>Cycle</summary>   
        <ul style='list-style-type: none;'>
            <li><a href="#cycle01">Cycle01</a></li>
            <li><a href='#cycle02'>Cycle02</a></li>
            <li><a href='#cycle03'>Cycle03</a></li>
            <li><a href='#cycle04'>Cycle04</a></li>
        </ul>
   </details>
   
6. **ê²°ë¡ **
### 1. ê°€ì„¤ ì„¤ì •

#### ê°€ì„¤ 1: ë¼ì´í”„ìŠ¤íƒ€ì¼ ìš”ì†Œì™€ ì‚¶ì˜ ë§Œì¡±ë„ ì ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„

-   **ê°€ì„¤ ë‚´ìš©**  
    íŠ¹ì • ë¼ì´í”„ìŠ¤íƒ€ì¼ ìš”ì†Œ(ì˜ˆ: ì‹ ì²´ í™œë™ ìˆ˜ì¤€, ì‚¬íšŒì  í™œë™, ì¼ê³¼ ìƒí™œì˜ ê· í˜• ë“±)ëŠ” ì‚¶ì˜ ë§Œì¡±ë„ ì ìˆ˜ì™€ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì¼ ê²ƒì´ë‹¤.  
    ì´ ê°€ì„¤ì€ ë¼ì´í”„ìŠ¤íƒ€ì¼ì´ ê°œì¸ì˜ ì¼ë°˜ì ì¸ ì›°ë¹™ê³¼ ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆë‹¤ëŠ” ê°€ì •ì— ê¸°ë°˜í•œë‹¤.

#### ê°€ì„¤ 2: ê¸ì •ì  ìš”ì†Œë“¤ì´ ì‚¶ì˜ ë§Œì¡±ë„ì— ë” í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤

-   **ê°€ì„¤ ë‚´ìš©**  
    ê¸ì •ì  ìš”ì†Œ(ì˜ˆ: ìê¸°ê³„ë°œ, ëª©í‘œ ë‹¬ì„±, ì‚¬íšŒì  ì§€ì›)ëŠ” ë¶€ì •ì  ìš”ì†Œ(ì˜ˆ: ìŠ¤íŠ¸ë ˆìŠ¤, ì—…ë¬´ ê³¼ë¶€í•˜)ë³´ë‹¤ ì‚¶ì˜ ë§Œì¡±ë„ì— ë” í° ê¸ì •ì  ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì´ë‹¤.  
    ì´ ê°€ì„¤ì€ ê¸ì •ì ì¸ ì‹¬ë¦¬ì , ì‚¬íšŒì  ìì›ì´ ë¶€ì •ì ì¸ ì˜í–¥ì„ ìƒì‡„í•˜ê³ , ì „ë°˜ì ì¸ ë§Œì¡±ë„ë¥¼ ë†’ì¸ë‹¤ëŠ” ì´ë¡ ì—ì„œ ë¹„ë¡¯ëœë‹¤.

#### ê°€ì„¤ ê²€ì¦ ë°©ë²•

1. **ë°ì´í„° ìˆ˜ì§‘**: ì„¤ë¬¸ì¡°ì‚¬, ì¸í„°ë·°, ê¸°ì¡´ ì—°êµ¬ ìë£Œ ë“±ì„ í†µí•´ ì‚¶ê³¼ ì¼ì˜ ë§Œì¡±ë„ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì†Œì— ëŒ€í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

2. **í†µê³„ì  ë¶„ì„**: ìˆ˜ì§‘ëœ ë°ì´í„°ì— ëŒ€í•´ ìƒê´€ê´€ê³„ ë¶„ì„, íšŒê·€ ë¶„ì„ ë“±ì„ ì‹¤ì‹œí•˜ì—¬ ê° ìš”ì†Œê°€ ë§Œì¡±ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì˜ ì •ë„ì™€ ë°©í–¥ì„ íŒŒì•…í•©ë‹ˆë‹¤.

3. **ëª¨ë¸ë§ê³¼ ìµœì í™”**: ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ ëª¨ë¸ì„ ìƒì„±í•˜ê³ , ì´ ëª¨ë¸ì„ ìµœì í™”í•˜ì—¬ ê°€ì„¤ì˜ íƒ€ë‹¹ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.  
   ì´ëŸ¬í•œ ê³¼ì •ì„ í†µí•´ ì„¤ì •ëœ ê°€ì„¤ì˜ íƒ€ë‹¹ì„±ì„ ê²€ì¦í•˜ê³ , ì¼ê³¼ ì‚¶ì˜ ë§Œì¡±ë„ë¥¼ ë†’ì´ëŠ” ë° ì¤‘ìš”í•œ ìš”ì†Œë¥¼ ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
   ì´ ê²°ê³¼ëŠ” ê°œì¸ê³¼ ì¡°ì§ì˜ ì›°ë¹™ ê°œì„  ì „ëµ ìˆ˜ë¦½ì— ì¤‘ìš”í•œ ê¸°ì—¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<hr>

### 2. ë°ì´í„° ë¶„ì„

```
# ë°ì´í„° ì…‹ ì •ë³´ í™•ì¸.
pre_l_df = l_df.copy()
pre_l_df.info()

# ê²°ì¸¡ì¹˜ í™•ì¸.
pre_l_df.isna().sum()

# daliy stress ì´ìƒê°’ ì‚­ì œ.
pre_l_df = pre_l_df[pre_l_df['DAILY_STRESS'] != '1/1/00']
pre_l_df = pre_l_df.reset_index(drop=True)
pre_l_df

# ë°ì´íƒ¸ ì…‹ ì •ë³´ í™•ì¸.
pre_l_df.info()

```

### 3. ë°ì´í„° ì „ì²˜ë¦¬

```
# ì¤‘ë³µê°’ í™•ì¸/
pre_l_df = pre_l_df.drop_duplicates().reset_index(drop=True)

# ìƒê´€ê´€ê³„ í™•ì¸
pre_l_df.corr()['WORK_LIFE_BALANCE_SCORE'].sort_values(ascending=False)[1:]

- ì´ë¯¸ì§€ íšŒê·€ì„  ë„£ê¸°
- corr ì´ë¯¸ì§€ ë„£ê¸°
- ìƒê´€ê´€ê³„ ì´ë¯¸ì§€ ë„—ê¸°
- ì„ í˜• ì´ë¯¸ì§€ ë„£ê¸°
- ì–‘ì˜ ìƒê´€ê´€ê³„ ìŒì˜ ìƒê´€ê´€ê³„ ë„£ê¸°
- ë‹¤ì¤‘ê³µì‚°ì„± ì§€í‘œ ë„£ê¸°
- OLS ì§€í‘œ ë„£ê¸°

```

### 4. ë°ì´í„° í›ˆë ¨

<h2 id="cycle01">Cycle01</h2>
<p>1. ë³„ë„ì˜ í›ˆë ¨ ì—†ì´ ì„ í˜• ë°ì´í„° í›ˆë ¨</p>

```
 ì„ í˜• ë°ì´í„° í›ˆë ¨
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

features, targets = pre_l_df.iloc[:, :-1], pre_l_df.iloc[:, -1]

X_train, X_test, y_train, y_test = \
train_test_split(features, targets, test_size=0.2, random_state=124)

l_r = LinearRegression()
l_r.fit(X_train, y_train)
```

```
import numpy as np
from sklearn.metrics import mean_squared_log_error, mean_squared_error, r2_score

def get_evaluation(y_test, prediction):
    MSE = mean_squared_error(y_test, prediction)
    RMSE = np.sqrt(MSE)
    MSLE = mean_squared_log_error(y_test, prediction)
    RMSLE = np.sqrt(mean_squared_log_error(y_test, prediction))
    R2 = r2_score(y_test, prediction)
    print('MSE: {:.4f}, RMSE: {:.4f}, MSLE: {:.4f}, RMSLE: {:.4f}, R2: {:.4f}'\
          .format(MSE, RMSE, MSLE, RMSLE, R2))

```

```
# í›ˆë ¨ ë°ì´í„° í‰ê°€ í•¨ìˆ˜.
import numpy as np
from sklearn.metrics import mean_squared_log_error, mean_squared_error, r2_score

def get_evaluation(y_test, prediction):
    MSE = mean_squared_error(y_test, prediction)
    RMSE = np.sqrt(MSE)
    MSLE = mean_squared_log_error(y_test, prediction)
    RMSLE = np.sqrt(mean_squared_log_error(y_test, prediction))
    R2 = r2_score(y_test, prediction)
    print('MSE: {:.4f}, RMSE: {:.4f}, MSLE: {:.4f}, RMSLE: {:.4f}, R2: {:.4f}'\
          .format(MSE, RMSE, MSLE, RMSLE, R2))
```

```
# í›ˆë ¨ ê²°ê³¼ Cycle_01
prediction = l_r.predict(X_test)
get_evaluation(y_test, prediction)
```

<h2 id="cycle02">Cycle02</h2>
<p>1. ì°¨ì› ì¶•ì†Œ ì§„í–‰ (PCA)</p>
<p>2. ì„ í˜• ë°ì´í„° í›ˆë ¨</p>

```
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA


for i in range(12):
    pca = PCA(n_components=(i+1))

    pca_train = pca.fit_transform(features)

    print(pca.explained_variance_ratio_.sum(), i)
```

```
# ë³´ì¡´ë¥ 
print(pca.explained_variance_ratio_)
print(pca.explained_variance_ratio_.sum())

0.9106306179404671
```

```
# íŒŒì´í”„ë¼ì¸ êµ¬ì¶• í›„ ì°¨ì› ì¶•ì†Œ í›„ ì„ í˜• íšŒê·€ ë¶„ì„
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline

features, targets = pre_l_df.iloc[:,:-1], pre_l_df.iloc[:,-1]

X_train, X_test, y_train, y_test = \
train_test_split(features,targets, test_size=0.2, random_state=321)

pipe = Pipeline(
    [
        ('pca', PCA(n_components=8)),
        ('l_r',LinearRegression())
    ]
)

pipe.fit(X_train, y_train)
```

```
prediction = pipe.predict(X_test)
get_evaluation(y_test, prediction)

MSE: 221.7647, RMSE: 14.8918, MSLE: 0.0005, RMSLE: 0.0227, R2: 0.8917
```

<h2 id="cycle03">Cycle03</h2>
<p>1. ê³¼ì í•©ì„ íŒë‹¨í•˜ê¸° ìœ„í•´ êµì°¨ê²€ì¦ ì§„í–‰</p>
 


```
from sklearn.model_selection import cross_val_score, KFold

features, targets = pre_l_df.iloc[:,:-1], pre_l_df.iloc[:,-1]

kf = KFold(n_splits=10, random_state=321, shuffle=True)
scores = cross_val_score(pipe, features, targets , cv=kf)
scores
>
array([0.89424997, 0.88934348, 0.89652179, 0.88739977, 0.88346597,
       0.88784766, 0.8909993 , 0.88743523, 0.88928659, 0.88422674])
```

```
# íŒŒì´í”„ë¼ì¸ êµ¬ì¶• í›„ ì°¨ì› ì¶•ì†Œ í›„ ì„ í˜• íšŒê·€ ë¶„ì„
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.pipeline import Pipeline


features, targets = pre_l_df.iloc[:,:-1], pre_l_df.iloc[:,-1]

X_train, X_test, y_train, y_test = \
train_test_split(features,targets, test_size=0.2, random_state=321)

kfold = KFold(n_splits=15, random_state=321, shuffle=True)

pipe = Pipeline(
    [
        ('pca', PCA(n_components=8)),  # PCAë¡œ ì°¨ì› ì¶•ì†Œ
        ('l_r', LinearRegression())    # ì„ í˜• íšŒê·€ ëª¨ë¸
    ]
)

param_grid = {
    'pca__n_components': [8]  # PCA ì»´í¬ë„ŒíŠ¸ ìˆ˜ ì¡°ì •
}

grid_l = GridSearchCV(pipe, param_grid=param_grid, cv=kfold, scoring='neg_mean_squared_error')
grid_l.fit(X_train, y_train)

# ìµœì ì˜ íŒŒë¼ë¯¸í„°ì™€ ì„±ëŠ¥ ì¶œë ¥
print("Best parameters:", grid_l.best_params_)
print("Best cross-validation score: {:.3f}".format(-grid_l.best_score_))

prediction = grid_l.predict(X_test)
get_evaluation(y_test, prediction)

MSE: 221.7647, RMSE: 14.8918, MSLE: 0.0005, RMSLE: 0.0227, R2: 0.8917
```

-   Cycle03
    -   êµì°¨ ê²€ì¦ ì§„í–‰ ì‹œ ê³¼ì í•© ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•œ ê·¸ë˜í”„ í™•ì¸
    
```
MSE: 226.5833, RMSE: 15.0527, MSLE: 0.0005, RMSLE: 0.0229, R2: 0.8874
MSE: 224.9903, RMSE: 14.9997, MSLE: 0.0005, RMSLE: 0.0228, R2: 0.8904

- ì„ í˜• ê·¸ë˜í”„ ì²¨ë¶€í•  ê²ƒ.

import matplotlib.pyplot as plt

prediction = grid_l.predict(X_test)
get_evaluation(y_test, prediction)

fig, ax = plt.subplots()
ax.scatter(y_test, prediction, edgecolors='red', c='orange', alpha=0.2)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')
plt.show()

- ì„ í˜• ê²°ê³¼ ê·¸ë˜í”„ ì²¨ë¶€í•  ê²ƒ.
```

-   Cycle04
    -   íŒŒì´í† ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ loss ê°’ì„ í™•ì¸í•˜ì—¬ ê·¸ë˜í”„ë¥¼ í™•ì¸í•˜ì—¬ ì‹¤ì œ train ì˜ ê°’ê³¼ test ì˜ ê°’ì´ ì˜¤ì°¨ê°€ ì»¤ì§€ëŠ” ì§€ì ì„ ì°¾ì•„ì„œ, ê·œì œë¥¼ í™•ì¸í•œë‹¤.

```
import torch
from torch.optim import SGD
from torch.nn.functional import mse_loss
from torch.nn import Linear

torch.manual_seed(321)

features, targets = pre_l_df.iloc[:,:-1], pre_l_df.iloc[:,-1]

X_train, X_test, y_train, y_test = \
train_test_split(features,targets, test_size=0.2, random_state=321)

real_X_train, val_X_test, real_y_train, val_y_test = \
train_test_split(X_train, y_train, test_size=0.5, random_state=321)

real_X_train = torch.FloatTensor(X_train.values)
real_y_train = torch.FloatTensor(y_train.values).view(-1, 1)

val_X_test = torch.FloatTensor(X_test.values)
val_y_test = torch.FloatTensor(y_test.values).view(-1, 1)
l_r = Linear(real_X_train.shape[1], 1)  # ì…ë ¥ ì°¨ì› ë™ì  í• ë‹¹
optimizer = SGD(l_r.parameters(), lr=0.001)  # ë³´ë‹¤ ì‹¤ìš©ì ì¸ í•™ìŠµë¥ 

epochs = 1000000  # ì ì ˆí•œ ì—í¬í¬ ìˆ˜
real_train_loss_history = []
val_test_loss_history = []

for epoch in range(1, epochs + 1):
    l_r.train()
    H = l_r(real_X_train)
    train_loss = mse_loss(real_y_train, H)

    optimizer.zero_grad()
    train_loss.backward()
    optimizer.step()

    l_r.eval()
    with torch.no_grad():
        H_test = l_r(val_X_test)
        test_loss = mse_loss(val_y_test, H_test)

    real_train_loss_history.append(train_loss.item())
    val_test_loss_history.append(test_loss.item())

    # if epoch % 10000 == 0:
    #     print(f'{epoch}/{epochs}: ', end='')
    #     W = l_r.weight.data.squeeze()
    #     b = l_r.bias.data
    #     for i, w in enumerate(W):
    #         print(f'W{i+1}: {w:.4f}, ', end='')
    #     print(f'b: {b.item():.4f}, Loss: {train_loss.item():.4f}\n')
    if epoch % 10000 == 0:
        print(f'{epoch}/{epochs}: ', end='')
        W = l_r.weight.data.squeeze()
        b = l_r.bias.data
        for i, w in enumerate(W):
            print(f'W{i+1}: {w:.4f}, ', end='')
        print(f'b: {b.item():.4f}, Loss: {test_loss.item():.4f}\n')

plt.figure(figsize=(10, 5))
plt.plot(real_train_loss_history, label='Training Loss', linewidth=3, color= 'skyblue')
plt.plot(val_test_loss_history, label='Val Loss', linestyle='--' , linewidth=3, alpha=0.3, color='red')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.title('Loss Trend Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

- ì†ì‹¤í•¨ìˆ˜ ê·¸ë˜í”„ ì²¨ë¶€ í•„ìš”.
```

-   Cycle05
    -   íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ì—¬ ì°¨ì›ì¶•ì†Œ ì§„í–‰ í›„ L2 ê·œì œ ì‚¬ìš©ëœ ì„ í˜• íšŒê·€ ë¶„ì„ ì§„í–‰.

```
from sklearn.linear_model import Lasso
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline

features, targets = pre_l_df.iloc[:,:-1], pre_l_df.iloc[:,-1]

X_train, X_test, y_train, y_test = \
train_test_split(features,targets, test_size=0.2, random_state=321)

pipe = Pipeline(
    [
        ('pca', PCA(n_components=8)),
        ('lasso', Lasso(alpha=50, max_iter=10000))
    ]
)
# pipe.fit(X_train, y_train)

param_grid = {
    'pca__n_components': [8]  # PCA ì»´í¬ë„ŒíŠ¸ ìˆ˜ ì¡°ì •
}

kfold = KFold(n_splits=15, random_state=321, shuffle=True)

grid_l = GridSearchCV(pipe, param_grid=param_grid, cv=kfold, scoring='neg_mean_squared_error')
grid_l.fit(X_train, y_train)

# ìµœì ì˜ íŒŒë¼ë¯¸í„°ì™€ ì„±ëŠ¥ ì¶œë ¥
print("Best parameters:", grid_l.best_params_)
print("Best cross-validation score: {:.3f}".format(-grid_l.best_score_))


prediction = grid_l.predict(X_test)
get_evaluation(y_test, prediction)

MSE: 420.1151, RMSE: 20.4967, MSLE: 0.0010, RMSLE: 0.0314, R2: 0.7949


- ì†ì‹¤í•¨ìˆ˜ ê·¸ë˜í”„ ì²¨ë¶€ í•„ìš”.
```

```
ìµœì¢… ê° R2 Scoreì— ëŒ€í•œ ë§‰ëŒ€ê·¸ë˜í”„ ì  ì²¨ë¶€ í•„ìš”.
```

-   ê²°ê³¼

    -   ì´ˆê¸° ë¶„ì„ì—ì„œ ì„ í˜• íšŒê·€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í›ˆë ¨ì‹œì¼°ì„ ë•Œ, ì˜ˆìƒë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.  
        ì´ë¥¼ í†µí•´ ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ë§Œì¡±ë„ ì‚¬ì´ì˜ ê°•í•œ ì„ í˜• ê´€ê³„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.  
        ê·¸ëŸ¬ë‚˜ ëª¨ë“  ë°ì´í„°ê°€ ìœ ìš©í•œ ê²ƒì€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì°¨ì› ì¶•ì†Œë¥¼ í†µí•´ ë°ì´í„°ì˜ í¬ì†Œì„±ì„ ë†’ì´ê³  ì¤‘ìš”í•œ íŠ¹ì„±ë§Œì„ ê°•ì¡°í–ˆìŠµë‹ˆë‹¤.

    -   ì´ì–´ì§„ ê³¼ì •ì—ì„œëŠ” êµì°¨ ê²€ì¦ì„ í™œìš©í•´ ëª¨ë¸ì˜ ê³¼ì í•© ì •ë„ë¥¼ ì ê²€í•˜ì˜€ìŠµë‹ˆë‹¤.  
        ê³¼ì í•©ì„ ê´€ë¦¬í•˜ê¸° ìœ„í•´ L2 ê·œì œë¥¼ ì ìš©, R2 ì ìˆ˜ë¥¼ ì¡°ì ˆí•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ê°•í™”í–ˆìŠµë‹ˆë‹¤.  
        ì´ëŸ¬í•œ ì ‘ê·¼ì€ ëª¨ë¸ì´ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ë” ì˜ ì¼ë°˜í™”í•˜ë„ë¡ ë„ì™€ì£¼ì—ˆìŠµë‹ˆë‹¤.

-   ê²°ë¡ 

    -   íšŒê·€ ë¶„ì„ì„ í†µí•´ ë‹¤ì–‘í•œ ìš”ì†Œë“¤ê³¼ ë§Œì¡±ë„ ì ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ì‹ë³„í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.  
        ë¶„ì„ ê²°ê³¼, ê¸ì •ì  ìš”ì†Œë“¤ì€ ë§Œì¡±ë„ ì ìˆ˜ì™€ ë†’ì€ ì–‘ì˜ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ëŠ” ë°˜ë©´, ì‚¶ì˜ ë§Œì¡±ë„ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìŒìˆ˜ì  ìš”ì†Œë“¤ì€ ìƒëŒ€ì ìœ¼ë¡œ ì˜í–¥ë ¥ì´ ë‚®ì•˜ìŠµë‹ˆë‹¤.  
        ì´ëŠ” ê¸ì •ì ì¸ ìš”ì†Œë“¤ì´ ê°œì¸ì˜ ì‚¶ì˜ ì§ˆì„ ë†’ì´ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

    -   ì´ëŸ¬í•œ ë¶„ì„ì„ í†µí•´ ì–»ì€ í†µì°°ë ¥ì€ ê°œì¸ê³¼ ì¡°ì§ì´ ì›°ë¹™ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì´ ê²°ê³¼ëŠ” ì •ì±… ê²°ì •ìë‚˜ ê±´ê°• ì „ë¬¸ê°€ë“¤ì—ê²Œë„ ìœ ìš©í•œ ë°ì´í„°ë¥¼ ì œê³µí•˜ì—¬, ë³´ë‹¤ íš¨ê³¼ì ì¸ ê±´ê°• ë° ì›°ë¹™ ê´€ë ¨ ì •ì±…ì„ ê°œë°œí•˜ëŠ” ê¸°ë°˜ì„ ë§ˆë ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
