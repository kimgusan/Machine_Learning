# â‘  Regression01

## ì£¼ì œ: # ğŸ¥ ë‰´ìš•ì£¼ë¦½ë³‘ì› ì…ì›í™˜ì í‡´ì› ê¸ˆì•¡

    (1) ë°ì´í„° ì›ë³¸: ex) https://kaggle.com)

### ëª©ì°¨

1. **ê°€ì„¤ ì„¤ì •**
2. **ë°ì´í„° ë¶„ì„**
3. **ë°ì´í„° ì „ì²˜ë¦¬**
4. **ë°ì´í„° í›ˆë ¨**
    - Cycle (n)ë°˜ë³µ - ê° ì´ë¯¸ì§€ì— ëŒ€í•˜ì—¬ ë§í¬ë¥¼ ê±¸ì–´ì„œ í™•ì¸í•  ê²ƒ.
5. **ê²°ë¡ **

## 1. ê°€ì„¤ ì„¤ì •

### ê°€ì„¤ 1: ì˜ë£Œ ì„œë¹„ìŠ¤ ìƒê´€ê´€ê³„ ë¶„ì„

-   **ëª©ì **: íŠ¹ì • ì§„ë‹¨ ì½”ë“œ ë˜ëŠ” ì ˆì°¨ ì½”ë“œì— ë”°ë¼ ë” ë§ì€ ì˜ë£Œ ë³´ì¥ì´ í•„ìš”í•œ ì§€ì—­ì„ ì‹ë³„í•©ë‹ˆë‹¤.
-   **ë°©ë²•**: ë‹¤ì–‘í•œ ì§€ë¶ˆ ì†ŒìŠ¤ ê°„ì˜ ìƒê´€ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬, ë³‘ì›ì´ ì…ì› í™˜ì ë°©ë¬¸ ì¤‘ ë¹„ìš© íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ìì› í• ë‹¹ì„ ìµœì í™”í•©ë‹ˆë‹¤.
-   **ì‘ìš©**: Medicaidì™€ ë¯¼ê°„ ë³´í—˜ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì§€ë¶ˆ ì†ŒìŠ¤ ê°„ì˜ íŒ¨í„´ì„ ì‹ë³„í•˜ì—¬, ì§€ì—­ë³„ ì˜ë£Œ ì„œë¹„ìŠ¤ì˜ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.

### ê°€ì„¤ 2: ìì „ê±° ìˆ˜ëŸ‰ ì˜ˆì¸¡ì„ í†µí•œ íš¨ìœ¨ì  ê´€ë¦¬ ê°€ëŠ¥ì„±

-   **í‡´ì› ê¸ˆì•¡ ìƒê´€ê´€ê³„**: ë³‘ì› í‡´ì› ì‹œ ì¸¡ì •ë˜ëŠ” ê¸ˆì•¡ê³¼ ë‹¤ì–‘í•œ ìš”ì†Œë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬, ê¸ˆì•¡ì— ëŒ€í•œ íšŒê·€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
-   **ì‘ìš© ê°€ëŠ¥ì„±**:
    -   **ì§„ë‹¨ ì½”ë“œì™€ ì˜ë£Œ ë³´ì¥**: í•„ìš”í•œ ì˜ë£Œ ë³´ì¥ì´ ë” ë§ì´ í•„ìš”í•œ ì§€ì—­ì„ ì§„ë‹¨í•˜ê³ , ì´ë¥¼ í†µí•´ ì˜ë£Œ ì„œë¹„ìŠ¤ì˜ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    -   **ë¹„ìš© íš¨ìœ¨ì„±ê³¼ ì§€ë¶ˆ ì†ŒìŠ¤ ë¶„ì„**: ë³‘ì›ì€ ë‹¤ì–‘í•œ ì§€ë¶ˆ ì†ŒìŠ¤ ê°„ì˜ ìƒê´€ ê´€ê³„ ë¶„ì„ì„ í†µí•´ ë¹„ìš© íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 2. ë°ì´í„° ë¶„ì„ ê²°ê³¼

-   **ì˜ˆì¸¡ ê°€ëŠ¥í•œ ëª¨ë¸ ê°œë°œ**: ë¶„ì„ì„ í†µí•´ ê°œë°œëœ ëª¨ë¸ì€ ì°¨ì› ì¶•ì†Œ í›„ì—ë„ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©°, ë³‘ì›ì˜ í‡´ì› ê¸ˆì•¡ì„ ì–´ëŠ ì •ë„ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
-   **ê³¼ì í•© ë¶€ì¬**: ê³¼ì í•©ì´ ê´€ì°°ë˜ì§€ ì•Šì•„, ëª¨ë¸ì— ì¶”ê°€ì ì¸ ê·œì œë¥¼ ì ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ íŠ¸ë¦¬ ê¸°ë°˜ íšŒê·€ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜í™”ëœ ê²°ê³¼ë¥¼ ì œê³µí•˜ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
-   **ì‚¬ìš©ëœ ê¸°ìˆ **:
    -   **íŠ¸ë¦¬ ê¸°ë°˜ íšŒê·€ ëª¨ë¸**: ë³µì¡í•œ ë°ì´í„° êµ¬ì¡°ì—ì„œ ìœ ì˜ë¯¸í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° íš¨ê³¼ì ì…ë‹ˆë‹¤.

<hr>

### 2. ë°ì´í„° ë¶„ì„

```

```

### 3. ë°ì´í„° ì „ì²˜ë¦¬

```
# ê²°ì¸¡ì¹˜ í™•ì¸
h_df.isna().sum()

# ì¤‘ë³µê°’ í™•ì¸
h_df.duplicated().sum()

# ë¶ˆí•„ìš” ì»¬ëŸ¼ ì‚­ì œ
columns = ['index','Health Service Area','Hospital County','Operating Certificate Number', 'Facility ID','Age Group', 'Gender', 'Race',
'Ethnicity','Length of Stay', 'Type of Admission', 'Patient Disposition','Discharge Year', 'CCS Diagnosis Code','CCS Procedure Code',
'APR DRG Code', 'APR MDC Code','APR Severity of Illness Code','APR Risk of Mortality','APR Medical Surgical Description','Attending Provider License Number',
'Operating Provider License Number','Other Provider License Number','Birth Weight','Abortion Edit Indicator',
'Emergency Department Indicator', 'Discharge Year', 'Total Charges']

pre_h_df = h_df[columns].copy()
pre_h_df

pre_h_df = pre_h_df.drop(labels = ['index','Discharge Year', 'Abortion Edit Indicator'], axis =1)

# ê²°ì¸¡ì¹˜ ì‚­ì œ
pre_h_df = pre_h_df.dropna().reset_index(drop=True)

# ì¤‘ë³µê°’ ì‚­ì œ
pre_h_df = pre_h_df.drop_duplicates().reset_index(drop=True)


# ì •ë³´ í™•ì¸ í›„ ë²”ì£¼í˜• ë°ì´í„° ë¶„ë¦¬
pre_h_df.info()


# ë²”ì£¼í˜• ë°ì´í„° ë¶„ë¦¬

category_h_df = pre_h_df.select_dtypes(include=['object']).copy()

# int, float ë°ì´í„° ë¶„ë¦¬
numeric_h_df = pre_h_df.select_dtypes(include=['int64', 'float64']).copy()


# Label encoding
from sklearn.preprocessing import LabelEncoder

columns = category_h_df.columns
encoders = {}

for column in columns:
    encoder = LabelEncoder()
    # ê° ì»¬ëŸ¼ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    category_h_df[column] = encoder.fit_transform(category_h_df[column].tolist())
    encoders[column] = encoder.classes_

category_h_df.sort_index(inplace=True)
numeric_h_df.sort_index(inplace=True)

# # ë²”ì£¼í˜•ê³¼ ìˆ˜ì¹˜í˜• ë°ì´í„° í”„ë ˆì„ì„ ê²°í•©í•©ë‹ˆë‹¤.
num_h_df = pd.concat([category_h_df, numeric_h_df], axis=1)
num_h_df



# ì´ìƒì¹˜ ì‚­ì œ ì§„í–‰ì„ ìœ„í•œ ì •ê·œí™” ì‘ì—…
from sklearn.preprocessing import StandardScaler

std = StandardScaler()
result = std.fit_transform(num_h_df)
std_num_h_df = pd.DataFrame(result, columns=num_h_df.columns)
std_num_h_df

condition = True
error_count = []

for column in std_num_h_df.columns:
    # í˜„ì¬ ì»¬ëŸ¼ì— ëŒ€í•´ -1.96ê³¼ 1.96 ì‚¬ì´ì— ì†í•˜ëŠ” ê°’ì„ ì¹´ìš´íŠ¸í•©ë‹ˆë‹¤.
    count = std_num_h_df[column].between(-1.96, 1.96).sum()
    error_count.append(count)
    condition &= std_num_h_df[column].between(-1.96, 1.96)

std_num_h_df = std_num_h_df[condition]
std_num_h_df

for column, count in zip(std_num_h_df.columns, error_count):
    # ì´ìƒì¹˜ì˜ ê°œìˆ˜ëŠ” ì „ì²´ ë°ì´í„° ê°œìˆ˜ì—ì„œ ì •ìƒì ì¸ ê°’ì˜ ê°œìˆ˜ë¥¼ ë¹¼ë©´ ë©ë‹ˆë‹¤.
    outlier_count = len(std_num_h_df) - count
    outlier_ratio = (outlier_count / len(std_num_h_df)) * 100
    print(f"'{column}'ì— ëŒ€í•œ ì´ìƒì¹˜ ê°œìˆ˜: {outlier_count},\n {column}ì— ëŒ€í•œ ì´ìƒì¹˜ ë¹„ìœ¨'{round(outlier_ratio,2)}%'\n")


# ì´ìƒì¹˜ ì œê±°
condition = True
error_count = []

for column in std_num_h_df.columns:
    if std_num_h_df[column].between(-1.96, 1.96) is True:
        error_count.append(std_num_h_df[column].between(-1.96, 1.96).count())
    condition &= std_num_h_df[column].between(-1.96, 1.96)

std_num_h_df = std_num_h_df[condition]
std_num_h_df


# ì´ìƒì¹˜ ì œê±°í•œ ë°ì´í„°ë¥¼ ì¸ë±ìŠ¤ ë²ˆí˜¸ì— ë§ê²Œ ê°€ì ¸ì˜¤ê¸°
numeric_h_df = numeric_h_df.iloc[std_num_h_df.index].reset_index(drop=True)
numeric_h_df

num_h_df['Total Charges'] = np.log1p(num_h_df['Total Charges'])

```

### 4. ë°ì´í„° í›ˆë ¨

-   Cycle01
    1. íƒ€ê²Ÿ ë°ì´í„° ë¶„í¬ê°€ ì¼ì •í•˜ì—¬ ì°¨ì› ì¶•ì†Œ ì—†ì´ ë¶„ì„ ì§„í–‰

```
- ì „ì²´ ê·¸ë˜í”„ì— ëŒ€í•´ì„œ ì²¨ë¶€

# íšŒê·€ ë¶„ì„ ëª¨ë¸ ì‚¬ìš©
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

features, targets = num_h_df.iloc[:, :-1], num_h_df.iloc[:, -1]

X_train, X_test, y_train, y_test = \
train_test_split(features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation(y_test, prediction)

MSE: 0.4996, RMSE: 0.7068, MSLE: 0.0041, RMSLE: 0.0644, R2: 0.5210
```

```
# ë¹„ì„ í˜• ëª¨ë¸ ì‚¬ìš©
# íšŒê·€ ë¶„ì„ ëª¨ë¸ ì‚¬ìš©
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures

poly_features = PolynomialFeatures(degree=2).fit_transform(features)

X_train, X_test, y_train, y_test = \
train_test_split(poly_features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 0.3935, RMSE: 0.6273, R2: 0.6228
```

```
- OLS ì§€í‘œ ì²¨ë¶€
```

```
# íŠ¸ë¦¬ ëª¨ë¸ í›ˆë ¨
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

features, targets = num_h_df.iloc[:, :-1],  num_h_df.iloc[:, -1]

X_train, X_test, y_train, y_test =\
train_test_split(features, targets, test_size=0.2, random_state=321)

dt_r = DecisionTreeRegressor(random_state=321)
rd_r = RandomForestRegressor(random_state=321)
gb_r = GradientBoostingRegressor(random_state=321)
xgb_r = XGBRegressor(random_state=321)
lgb_r = LGBMRegressor(random_state=321)

models = [dt_r, rd_r, gb_r, xgb_r, lgb_r]

for model in models:
    model.fit(X_train, y_train)
    prediction = model.predict(X_test)
    print(model.__class__.__name__)
    get_evaluation(y_test, prediction)
```

-   Cycle02
    1. ëª¨ë¸ì˜ ê³µë¶„ì‚°ì„±ì„ ì§€ë‹Œ ìˆ˜ì¹˜ ì¤‘ ë†’ì€ ìˆ˜ì¹˜ì— ëŒ€í•˜ì—¬ ì‚­ì œ í›„ í™•ì¸
    2. ëª¨ë¸ í›ˆë ¨ì†ë„ì˜ íš¨ìœ¨ì„ ë†’ì´ê¸° ìœ„í•´ ì°¨ì›ì¶•ì†Œë¥¼ ì§„í–‰.

```
# ìƒê´€ê´€ê³„ í™•ì¸
num_h_df.corr()['Total Charges'].sort_values(ascending=False)[1:]

import seaborn as sns
corr = num_h_df.corr()
sns.heatmap(corr, cmap='Oranges')
- íˆíŠ¸ë§µê·¸ë˜í”„ ì²¨ë¶€

```

```
from statsmodels.stats.outliers_influence import variance_inflation_factor

def get_vif(features):
    vif = pd.DataFrame()
    vif['vif_score'] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]
    vif['feature'] = features.columns
    return vif

get_vif(features)

- ë‹¤ì¤‘ê³µì„ ì„± ìˆ˜ì¹˜ ê·¸ë˜í”„ ì²¨ë¶€
```

```
# ë¶ˆí•„ìš” feature ì œê±°
c2_h_df = num_h_df.drop(labels = ['Operating Certificate Number', 'APR DRG Code', 'Health Service Area'], axis = 1)
```

```
# ì°¨ì›ì¶•ì†Œ ì§„í–‰
from sklearn.model_selection import train_test_split

features, targets = c2_h_df.iloc[:, :-1], c2_h_df.iloc[:, -1]

X_train, X_test, y_train, y_test = \
train_test_split(features, targets, test_size=0.2, random_state=124)


# ì†ì‹¤ìœ¨ í™•ì¸
from sklearn.decomposition import PCA

for i in range(4):
    pca = PCA(n_components=(i + 1))

    pca_train = pca.fit_transform(X_train)

    # ì†ì‹¤ìœ¨
    print(pca.explained_variance_ratio_.sum())
```

```
# ì°¨ì› ì¶•ì†Œ í›„ ìµœì ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ ì§„í–‰
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA

features, targets = c2_h_df.iloc[:, :-1], c2_h_df.iloc[:, -1]

X_train, X_test, y_train, y_test = \
train_test_split(features, targets, test_size=0.2, random_state=124)

l_r = LinearRegression()

# pipe = Pipeline([('pca', PCA(n_components=2)), ('l_r', l_r)])
# ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ë˜ ëª¨ë¸ë¡œ ì‚¬ìš©
pipe = Pipeline([('pca', PCA(n_components=8)), ('lgb_r', LGBMRegressor(random_state=321))])
pipe.fit(X_train, y_train)

prediction = pipe.predict(X_test)
get_evaluation_negative(y_test, prediction)

- í•´ë‹¹ ê²°ê³¼ê°’ì— ëŒ€í•˜ì—¬ ë³µì‚¬í•´ì„œ ë¶™ì—¬ë„£ì„ ê²ƒ.
```

```
# train, validation ê·¸ë˜í”„ì— ëŒ€í•˜ì—¬ ê²€ì¦
import matplotlib.pyplot as plt


r_X_train, v_X_train, r_y_train, v_y_train = \
train_test_split(X_train, y_train, test_size= 0.3, random_state=321)

r_X_train_prediction = pipe.predict(r_X_train)
get_evaluation_negative(r_y_train, r_X_train_prediction)

v_X_train_prediction = pipe.predict(v_X_train)
get_evaluation_negative(v_y_train, v_X_train_prediction)


fig, ax = plt.subplots(1, 2, figsize= (12, 5))

ax[0].scatter(r_y_train, r_X_train_prediction, edgecolors='red', c='red', alpha=0.2)
ax[0].plot([r_y_train.min(), r_y_train.max()], [r_y_train.min(), r_y_train.max()], 'k--')
ax[0].set_title('Train Data Prediction')

ax[1].scatter(v_y_train, v_X_train_prediction, edgecolors='red', c='blue', alpha=0.2)
ax[1].plot([v_y_train.min(), v_y_train.max()], [v_y_train.min(), v_y_train.max()], 'k--')
ax[1].set_title('Validation Data Prediction')
plt.show()

- ê·¸ë˜í”„ ì²¨ë¶€

import matplotlib.pyplot as plt

prediction = pipe.predict(X_test)
get_evaluation_negative(y_test, prediction)

fig, ax = plt.subplots()
ax.scatter(y_test, prediction, edgecolors='red', c='orange', alpha=0.2)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')
plt.show()

- í…ŒìŠ¤íŠ¸ ê·¸ë˜í”„ ì²¨ë¶€
```

-   Cycle03
    1. ëª¨ë¸ì˜ ì‹ ë¢°ì„±ì„ ë†’ì´ê¸° ìœ„í•´ êµì°¨ê²€ì¦

```
from sklearn.model_selection import cross_val_score, KFold

features, targets = c2_h_df.iloc[:,:-1], c2_h_df.iloc[:,-1]

kf = KFold(n_splits=10, random_state=321, shuffle=True)
scores = cross_val_score( lgb_r, features, targets , cv=kf)
scores

[0.89412771, 0.89391949, 0.89559464, 0.89305629, 0.89536895,
       0.89599536, 0.89029503, 0.89492653, 0.89381586, 0.89115958])
```

```
# íŒŒì´í”„ë¼ì¸ êµ¬ì¶• í›„ ì°¨ì› ì¶•ì†Œ í›„ ì„ í˜• íšŒê·€ ë¶„ì„
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.pipeline import Pipeline
from lightgbm import LGBMRegressor
from sklearn.preprocessing import StandardScaler


features, targets = c2_h_df.iloc[:,:-1], c2_h_df.iloc[:,-1]

X_train, X_test, y_train, y_test = \
train_test_split(features,targets, test_size=0.2, random_state=321)

kfold = KFold(n_splits=10, random_state=321, shuffle=True)


parameters = {
    # 'lgb_r__num_leaves': [10, 20, 30],
    # 'lgb_r__learning_rate': [0.05, 0.1, 0.15],
    # 'lgb_r__n_estimators': [50],
    # 'lgb_r__reg_lambda': [10000]  # L2 ê·œì œ ì¶”ê°€
    'lgb_r__random_state': [321]
}

pipe = Pipeline(
    [
        ('pca', PCA(n_components=8)),
        ('lgb_r', LGBMRegressor())
    ]
)

grid_lgb = GridSearchCV(pipe, param_grid=parameters, cv=kfold, scoring='r2')
grid_lgb.fit(X_train, y_train)

# ìµœì ì˜ íŒŒë¼ë¯¸í„°ì™€ ì„±ëŠ¥ ì¶œë ¥
print("Best parameters:", grid_lgb.best_params_)
print("Best cross-validation score: {:.3f}".format(grid_lgb.best_score_))

prediction = grid_lgb.predict(X_test)
get_evaluation_negative(y_test, prediction)

- MSE: 0.3039, RMSE: 0.5513, R2: 0.7087
```

```
# trian, validation ê·¸ë˜í”„ í™•ì¸
import matplotlib.pyplot as plt


r_X_train, v_X_train, r_y_train, v_y_train = \
train_test_split(X_train, y_train, test_size= 0.3, random_state=321)

r_X_train_prediction = grid_lgb.predict(r_X_train)
get_evaluation_negative(r_y_train, r_X_train_prediction)

v_X_train_prediction = grid_lgb.predict(v_X_train)
get_evaluation_negative(v_y_train, v_X_train_prediction)


fig, ax = plt.subplots(1, 2, figsize= (12, 5))

ax[0].scatter(r_y_train, r_X_train_prediction, edgecolors='red', c='red', alpha=0.2)
ax[0].plot([r_y_train.min(), r_y_train.max()], [r_y_train.min(), r_y_train.max()], 'k--')
ax[0].set_title('Train Data Prediction')

ax[1].scatter(v_y_train, v_X_train_prediction, edgecolors='red', c='blue', alpha=0.2)
ax[1].plot([v_y_train.min(), v_y_train.max()], [v_y_train.min(), v_y_train.max()], 'k--')
ax[1].set_title('Validation Data Prediction')
plt.show()

- ê·¸ë˜í”„ íŒŒì„ ì²¨ë¶€ ì˜ˆì •
```

```
# í…ŒìŠ¤íŠ¸ ê·¸ë˜í”„ í™•ì¸
import matplotlib.pyplot as plt

prediction = grid_lgb.predict(X_test)
get_evaluation_negative(y_test, prediction)

fig, ax = plt.subplots()
ax.scatter(y_test, prediction, edgecolors='red', c='orange', alpha=0.2)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')
plt.show()

- ê·¸ë˜í”„ íŒŒì¼ ì²¨ë¶€ ì˜ˆì •
```

```
bar ê·¸ë˜í”„ ì²¨ë¶€ ì§„í–‰
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ë°ì´í„° ì¤€ë¹„
data = {
    "Model_Cycle": [
        "Cycle01 Linear", "Cycle01 Poly", "Cycle01 DecisionTree",
        "Cycle01 RandomForest", "Cycle01 GradientBoosting", "Cycle01 XGB", "Cycle01 LGBM",
        "Cycle02 LGBM Train", "Cycle02 LGBM Validation", "Cycle02 LGBM Test",
        "Cycle03 LGBM Train", "Cycle03 LGBM Validation", "Cycle03 LGBM Test"
    ],
    "R2": [
        0.5210, 0.6228, 0.8324,
        0.9118, 0.8079, 0.9196, 0.9003,
        0.7131, 0.7127, 0.7071,
        0.7156, 0.7113, 0.7083
    ]
}

df = pd.DataFrame(data)

# ë§‰ëŒ€ ê·¸ë˜í”„ ì„¤ì •
plt.figure(figsize=(12, 8))  # ê·¸ë˜í”„ í¬ê¸° ì¡°ì ˆ
bar_plot = sns.barplot(x="Model_Cycle", y="R2", data=df, palette="viridis")

# ì‚¬ì´í´ë³„ ìƒ‰ìƒ ì§€ì •
colors = ['red', 'green', 'blue']
cycle_colors = {cycle: colors[i % len(colors)] for i, cycle in enumerate(df['Model_Cycle'].apply(lambda x: x.split()[0]).unique())}

for bar, color in zip(bar_plot.patches, df['Model_Cycle'].apply(lambda x: cycle_colors[x.split()[0]])):
    bar.set_color(color)  # ê° ë§‰ëŒ€ì— ìƒ‰ìƒ ì ìš©

# ê° ë§‰ëŒ€ì— R2 ì ìˆ˜ í…ìŠ¤íŠ¸ ì¶”ê°€
for bar in bar_plot.patches:
    bar_plot.annotate(format(bar.get_height(), '.4f'),
                      (bar.get_x() + bar.get_width() / 2, bar.get_height()),
                      ha='center', va='center',
                      xytext=(0, 9),
                      textcoords='offset points')

# ë ˆì´ë¸” ë° íƒ€ì´í‹€ ì„¤ì •
plt.xlabel("Model and Cycle")
plt.ylabel("R2 Score")
plt.title("R2 Scores Across Different Cycles and Models")
plt.xticks(rotation=45)  # xì¶• ë ˆì´ë¸” íšŒì „
plt.ylim(0.4, 1.0)  # yì¶• ë²”ìœ„ ì¡°ì •
plt.grid(True, linestyle='--', linewidth=0.5, color='gray', axis='y', zorder=0)

# ê·¸ë˜í”„ ë³´ì—¬ì£¼ê¸°
plt.tight_layout()
plt.show()

```

-   ê²°ê³¼

    ë¶„ì„ ê²°ê³¼:

    ì˜ˆì¸¡ ê°€ëŠ¥í•œ ëª¨ë¸ì˜ ê°œë°œ: ë¶„ì„ì„ í†µí•´ ê°œë°œëœ ëª¨ë¸ì€ ì—¬ëŸ¬ ìš”ì†Œë“¤ì„ ê³ ë ¤í•˜ì—¬ ë³‘ì›ì˜ í‡´ì› ê¸ˆì•¡ì„ ì–´ëŠ ì •ë„ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ëª¨ë¸ì€ ì°¨ì› ì¶•ì†Œ í›„ì—ë„ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê³¼ì í•©ì˜ ë¶€ì¬: ê³¼ì í•©ì´ ê´€ì°°ë˜ì§€ ì•Šì•„ ëª¨ë¸ì— ì¶”ê°€ì ì¸ ê·œì œë¥¼ ì ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•´ ì§€ë‚˜ì¹˜ê²Œ ìµœì í™”ë˜ì§€ ì•Šê³  ì¼ë°˜í™”ëœ ê²°ê³¼ë¥¼ ì œê³µí•˜ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì‚¬ìš©ëœ ê¸°ìˆ :

    íŠ¸ë¦¬ ê¸°ë°˜ íšŒê·€ ëª¨ë¸: íŠ¸ë¦¬ ê¸°ë°˜ì˜ íšŒê·€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì˜€ìœ¼ë©°, ì´ ëª¨ë¸ì€ ë³µì¡í•œ ë°ì´í„° êµ¬ì¡°ì—ì„œ ìœ ì˜ë¯¸í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° íš¨ê³¼ì ì´ì—ˆ
