# â‘¢ Regression03 (ë‹¤ì°¨ì›)

## ì£¼ì œ: # ğŸ¥ ë‰´ìš•ì£¼ë¦½ë³‘ì› ì…ì›í™˜ì í‡´ì› ê¸ˆì•¡

    (1) ë°ì´í„° ì›ë³¸: ex) https://kaggle.com)

### ëª©ì°¨

1. **ê°€ì„¤ ì„¤ì •**
2. **ë°ì´í„° ë¶„ì„**
3. **ë°ì´í„° ì „ì²˜ë¦¬**
4. **ë°ì´í„° í›ˆë ¨**
    <details>
        <summary>Cycle</summary>   
        <ul style='list-style-type: none;'>
            <li><a href="#cycle01">Cycle01(ì „ì²˜ë¦¬ ì´í›„ íšŒê·€ í›ˆë ¨)</a></li>
            <li><a href='#cycle02'>Cycle02(ë‹¤ì¤‘ê³µì„ ì„± ë° ìƒê´€ê´€ê³„ í™•ì¸ í›„ ë¶„ì„ íš¨ìœ¨ì„ ìœ„í•œ ì°¨ì› ì¶•ì†Œ ì§„í–‰)</a></li>
            <li><a href='#cycle03'>Cycle03(ëª¨ë¸ì˜ ì‹ ë¢°ì„±ì„ ìœ„í•œ êµì°¨ê²€ì¦ ì§„í–‰)</a></li>
        </ul>
   </details>
5. **ê²°ë¡ **

## 1. ê°€ì„¤ ì„¤ì •

### ê°€ì„¤ 1: ì˜ë£Œ ì„œë¹„ìŠ¤ ìƒê´€ê´€ê³„ ë¶„ì„

-   **í‡´ì› ê¸ˆì•¡ ìƒê´€ê´€ê³„**: ë³‘ì› í‡´ì› ì‹œ ì¸¡ì •ë˜ëŠ” ê¸ˆì•¡ê³¼ ë‹¤ì–‘í•œ ìš”ì†Œë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬, ê¸ˆì•¡ì— ëŒ€í•œ íšŒê·€ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
-   **ì‘ìš© ê°€ëŠ¥ì„±**:
    -   **ì§„ë‹¨ ì½”ë“œì™€ ì˜ë£Œ ë³´ì¥**: í•„ìš”í•œ ì˜ë£Œ ë³´ì¥ì´ ë” ë§ì´ í•„ìš”í•œ ì§€ì—­ì„ ì§„ë‹¨í•˜ê³ , ì´ë¥¼ í†µí•´ ì˜ë£Œ ì„œë¹„ìŠ¤ì˜ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    -   **ë¹„ìš© íš¨ìœ¨ì„±ê³¼ ì§€ë¶ˆ ì†ŒìŠ¤ ë¶„ì„**: ë³‘ì›ì€ ë‹¤ì–‘í•œ ì§€ë¶ˆ ì†ŒìŠ¤ ê°„ì˜ ìƒê´€ ê´€ê³„ ë¶„ì„ì„ í†µí•´ ë¹„ìš© íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<hr>

### 2. ë°ì´í„° ë¶„ì„

```
import pandas as pd

h_df = pd.read_csv('../../../datasets/p_hospital-inpatient-discharges-sparcs-de-identified-2010-1.csv', low_memory=False)
h_d

h_df.info()
```

### 3. ë°ì´í„° ì „ì²˜ë¦¬

<details>
  <summary>Click data preprocessing</summary>

```
# ê²°ì¸¡ì¹˜ í™•ì¸
h_df.isna().sum()

# ì¤‘ë³µê°’ í™•ì¸
h_df.duplicated().sum()

# ë¶ˆí•„ìš” ì»¬ëŸ¼ ì‚­ì œ
columns = ['index','Health Service Area','Hospital County','Operating Certificate Number', 'Facility ID','Age Group', 'Gender', 'Race',
'Ethnicity','Length of Stay', 'Type of Admission', 'Patient Disposition','Discharge Year', 'CCS Diagnosis Code','CCS Procedure Code',
'APR DRG Code', 'APR MDC Code','APR Severity of Illness Code','APR Risk of Mortality','APR Medical Surgical Description','Attending Provider License Number',
'Operating Provider License Number','Other Provider License Number','Birth Weight','Abortion Edit Indicator',
'Emergency Department Indicator', 'Discharge Year', 'Total Charges']

pre_h_df = h_df[columns].copy()
pre_h_df

pre_h_df = pre_h_df.drop(labels = ['index','Discharge Year', 'Abortion Edit Indicator'], axis =1)

# ê²°ì¸¡ì¹˜ ì‚­ì œ
pre_h_df = pre_h_df.dropna().reset_index(drop=True)

# ì¤‘ë³µê°’ ì‚­ì œ
pre_h_df = pre_h_df.drop_duplicates().reset_index(drop=True)


# ì •ë³´ í™•ì¸ í›„ ë²”ì£¼í˜• ë°ì´í„° ë¶„ë¦¬
pre_h_df.info()


# ë²”ì£¼í˜• ë°ì´í„° ë¶„ë¦¬

category_h_df = pre_h_df.select_dtypes(include=['object']).copy()

# int, float ë°ì´í„° ë¶„ë¦¬
numeric_h_df = pre_h_df.select_dtypes(include=['int64', 'float64']).copy()


# Label encoding
from sklearn.preprocessing import LabelEncoder

columns = category_h_df.columns
encoders = {}

for column in columns:
    encoder = LabelEncoder()
    # ê° ì»¬ëŸ¼ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    category_h_df[column] = encoder.fit_transform(category_h_df[column].tolist())
    encoders[column] = encoder.classes_

category_h_df.sort_index(inplace=True)
numeric_h_df.sort_index(inplace=True)

# # ë²”ì£¼í˜•ê³¼ ìˆ˜ì¹˜í˜• ë°ì´í„° í”„ë ˆì„ì„ ê²°í•©í•©ë‹ˆë‹¤.
num_h_df = pd.concat([category_h_df, numeric_h_df], axis=1)
num_h_df



# ì´ìƒì¹˜ ì‚­ì œ ì§„í–‰ì„ ìœ„í•œ ì •ê·œí™” ì‘ì—…
from sklearn.preprocessing import StandardScaler

std = StandardScaler()
result = std.fit_transform(num_h_df)
std_num_h_df = pd.DataFrame(result, columns=num_h_df.columns)
std_num_h_df

condition = True
error_count = []

for column in std_num_h_df.columns:
    # í˜„ì¬ ì»¬ëŸ¼ì— ëŒ€í•´ -1.96ê³¼ 1.96 ì‚¬ì´ì— ì†í•˜ëŠ” ê°’ì„ ì¹´ìš´íŠ¸í•©ë‹ˆë‹¤.
    count = std_num_h_df[column].between(-1.96, 1.96).sum()
    error_count.append(count)
    condition &= std_num_h_df[column].between(-1.96, 1.96)

std_num_h_df = std_num_h_df[condition]
std_num_h_df

for column, count in zip(std_num_h_df.columns, error_count):
    # ì´ìƒì¹˜ì˜ ê°œìˆ˜ëŠ” ì „ì²´ ë°ì´í„° ê°œìˆ˜ì—ì„œ ì •ìƒì ì¸ ê°’ì˜ ê°œìˆ˜ë¥¼ ë¹¼ë©´ ë©ë‹ˆë‹¤.
    outlier_count = len(std_num_h_df) - count
    outlier_ratio = (outlier_count / len(std_num_h_df)) * 100
    print(f"'{column}'ì— ëŒ€í•œ ì´ìƒì¹˜ ê°œìˆ˜: {outlier_count},\n {column}ì— ëŒ€í•œ ì´ìƒì¹˜ ë¹„ìœ¨'{round(outlier_ratio,2)}%'\n")


# ì´ìƒì¹˜ ì œê±°
condition = True
error_count = []

for column in std_num_h_df.columns:
    if std_num_h_df[column].between(-1.96, 1.96) is True:
        error_count.append(std_num_h_df[column].between(-1.96, 1.96).count())
    condition &= std_num_h_df[column].between(-1.96, 1.96)

std_num_h_df = std_num_h_df[condition]
std_num_h_df


# ì´ìƒì¹˜ ì œê±°í•œ ë°ì´í„°ë¥¼ ì¸ë±ìŠ¤ ë²ˆí˜¸ì— ë§ê²Œ ê°€ì ¸ì˜¤ê¸°
numeric_h_df = numeric_h_df.iloc[std_num_h_df.index].reset_index(drop=True)
numeric_h_df

num_h_df['Total Charges'] = np.log1p(num_h_df['Total Charges'])

```

</details>

    
<img width="540" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/7aa3e2e7-1be9-4180-8799-cded1a13e522">
<img width="398" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/d5bad4b0-6e48-4e06-bf09-50c0f028460a">

### 4. ë°ì´í„° í›ˆë ¨

<h2 id='cycle01'>Cycle01</h2>
<p>1. íƒ€ê²Ÿ ë°ì´í„° ë¶„í¬ê°€ ì¼ì •í•˜ì—¬ ì°¨ì› ì¶•ì†Œ ì—†ì´ ë¶„ì„ ì§„í–‰</p>


<details>
  <summary>Click Cycle01_code</summary>
    
```
# íšŒê·€ ë¶„ì„ ëª¨ë¸ ì‚¬ìš©
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

features, targets = num_h_df.iloc[:, :-1], num_h_df.iloc[:, -1]

X_train, X_test, y_train, y_test = \
train_test_split(features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation(y_test, prediction)

MSE: 0.4996, RMSE: 0.7068, MSLE: 0.0041, RMSLE: 0.0644, R2: 0.5210
```
<img width="354" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 11 11 31" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/357d68c7-4630-4822-943c-b4659ab4b934">

```
# ë¹„ì„ í˜• ëª¨ë¸ ì‚¬ìš©
# íšŒê·€ ë¶„ì„ ëª¨ë¸ ì‚¬ìš©
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures

poly_features = PolynomialFeatures(degree=2).fit_transform(features)

X_train, X_test, y_train, y_test = \
train_test_split(poly_features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 0.3935, RMSE: 0.6273, R2: 0.6228
```
<img width="225" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 11 11 34" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/5f0e53fe-c4d4-4653-a689-63bd9a20a650">


```
# íŠ¸ë¦¬ ëª¨ë¸ í›ˆë ¨
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

features, targets = num_h_df.iloc[:, :-1],  num_h_df.iloc[:, -1]

X_train, X_test, y_train, y_test =\
train_test_split(features, targets, test_size=0.2, random_state=321)

dt_r = DecisionTreeRegressor(random_state=321)
rd_r = RandomForestRegressor(random_state=321)
gb_r = GradientBoostingRegressor(random_state=321)
xgb_r = XGBRegressor(random_state=321)
lgb_r = LGBMRegressor(random_state=321)

models = [dt_r, rd_r, gb_r, xgb_r, lgb_r]

for model in models:
    model.fit(X_train, y_train)
    prediction = model.predict(X_test)
    print(model.__class__.__name__)
    get_evaluation(y_test, prediction)
```
</details>

<img width="546" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 11 11 43" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/3dd0457f-53ec-40cc-a9fe-0e3b2d5cde6e">
<img width="551" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 11 11 49" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/a5fd267a-bf7c-45d2-bc75-f93a11d139fa">

    
<h2 id='cycle02'>Cycle02</h2>
<p>1. ëª¨ë¸ì˜ ê³µë¶„ì‚°ì„±ì„ ì§€ë‹Œ ìˆ˜ì¹˜ ì¤‘ ë†’ì€ ìˆ˜ì¹˜ì— ëŒ€í•˜ì—¬ ì‚­ì œ í›„ í™•ì¸</p>
<p>2. ëª¨ë¸ í›ˆë ¨ì†ë„ì˜ íš¨ìœ¨ì„ ë†’ì´ê¸° ìœ„í•´ ì°¨ì›ì¶•ì†Œë¥¼ ì§„í–‰.</p>

<details>
  <summary>Click Cycle02_code</summary>

```
# ìƒê´€ê´€ê³„ í™•ì¸
num_h_df.corr()['Total Charges'].sort_values(ascending=False)[1:]

import seaborn as sns
corr = num_h_df.corr()
sns.heatmap(corr, cmap='Oranges')

from statsmodels.stats.outliers_influence import variance_inflation_factor

def get_vif(features):
    vif = pd.DataFrame()
    vif['vif_score'] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]
    vif['feature'] = features.columns
    return vif

get_vif(features)

# ë¶ˆí•„ìš” feature ì œê±°
c2_h_df = num_h_df.drop(labels = ['Operating Certificate Number', 'APR DRG Code', 'Health Service Area'], axis = 1)

```

<h4>before</h4>
<img width="512" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/f701ff24-270c-4292-8c26-4916fc217b57">
<img width="242" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/afeb8d47-ed45-42cc-9df3-bd681999e1c4">

<h4>after</h4>
<img width="230" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/7ed00221-7092-4635-9bc4-f394616364b1">
<img width="250" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/3facd9dd-8929-4cc2-b45a-07c78d123eb3">

```
# ì°¨ì›ì¶•ì†Œ ì§„í–‰
from sklearn.model_selection import train_test_split

features, targets = c2_h_df.iloc[:, :-1], c2_h_df.iloc[:, -1]

X_train, X_test, y_train, y_test = \
train_test_split(features, targets, test_size=0.2, random_state=124)


# ë³´ì¡´ìœ¨ í™•ì¸
from sklearn.decomposition import PCA

for i in range(4):
    pca = PCA(n_components=(i + 1))

    pca_train = pca.fit_transform(X_train)

    # ë³´ì¡´ìœ¨
    print(pca.explained_variance_ratio_.sum())
```
<img width="118" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/62101d32-56d4-4ece-a34d-30278d2e7c7d">

```
# ì°¨ì› ì¶•ì†Œ í›„ ìµœì ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ ì§„í–‰
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA

features, targets = c2_h_df.iloc[:, :-1], c2_h_df.iloc[:, -1]

X_train, X_test, y_train, y_test = \
train_test_split(features, targets, test_size=0.2, random_state=124)

l_r = LinearRegression()

# pipe = Pipeline([('pca', PCA(n_components=2)), ('l_r', l_r)])
# ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ë˜ ëª¨ë¸ë¡œ ì‚¬ìš©
pipe = Pipeline([('pca', PCA(n_components=8)), ('lgb_r', LGBMRegressor(random_state=321))])
pipe.fit(X_train, y_train)

prediction = pipe.predict(X_test)
get_evaluation_negative(y_test, prediction)

```
<img width="105" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/3620c683-d2ee-4324-aae4-d70c03ceac4c">
<img width="208" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/f92a5aad-62d5-4330-80fe-9ef57af55102">

```
# train, validation ê·¸ë˜í”„ì— ëŒ€í•˜ì—¬ ê²€ì¦
import matplotlib.pyplot as plt


r_X_train, v_X_train, r_y_train, v_y_train = \
train_test_split(X_train, y_train, test_size= 0.3, random_state=321)

r_X_train_prediction = pipe.predict(r_X_train)
get_evaluation_negative(r_y_train, r_X_train_prediction)

v_X_train_prediction = pipe.predict(v_X_train)
get_evaluation_negative(v_y_train, v_X_train_prediction)


fig, ax = plt.subplots(1, 2, figsize= (12, 5))

ax[0].scatter(r_y_train, r_X_train_prediction, edgecolors='red', c='red', alpha=0.2)
ax[0].plot([r_y_train.min(), r_y_train.max()], [r_y_train.min(), r_y_train.max()], 'k--')
ax[0].set_title('Train Data Prediction')

ax[1].scatter(v_y_train, v_X_train_prediction, edgecolors='red', c='blue', alpha=0.2)
ax[1].plot([v_y_train.min(), v_y_train.max()], [v_y_train.min(), v_y_train.max()], 'k--')
ax[1].set_title('Validation Data Prediction')
plt.show()

import matplotlib.pyplot as plt

prediction = pipe.predict(X_test)
get_evaluation_negative(y_test, prediction)

fig, ax = plt.subplots()
ax.scatter(y_test, prediction, edgecolors='red', c='orange', alpha=0.2)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')
plt.show()
```

<img width="670" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 11 16 04" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/345265cd-fbf5-4b1b-af63-edb03486e2b6">
<img width="378" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 11 16 08" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/e453e9ac-7a40-426f-8363-9e5ff58f44dc">

</details>


<h2 id='cycle03'>Cycle03</h2>
<p>1. ëª¨ë¸ì˜ ì‹ ë¢°ì„±ì„ ë†’ì´ê¸° ìœ„í•´ êµì°¨ê²€ì¦</p>

<details>
  <summary>Click Cycle03_code</summary>

```
from sklearn.model_selection import cross_val_score, KFold

features, targets = c2_h_df.iloc[:,:-1], c2_h_df.iloc[:,-1]

kf = KFold(n_splits=10, random_state=321, shuffle=True)
scores = cross_val_score( lgb_r, features, targets , cv=kf)
scores
```
<img width="360" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/33ba3740-380f-4c96-a7da-4f5557881116">

```
# íŒŒì´í”„ë¼ì¸ êµ¬ì¶• í›„ ì°¨ì› ì¶•ì†Œ í›„ ì„ í˜• íšŒê·€ ë¶„ì„
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.pipeline import Pipeline
from lightgbm import LGBMRegressor
from sklearn.preprocessing import StandardScaler


features, targets = c2_h_df.iloc[:,:-1], c2_h_df.iloc[:,-1]

X_train, X_test, y_train, y_test = \
train_test_split(features,targets, test_size=0.2, random_state=321)

kfold = KFold(n_splits=10, random_state=321, shuffle=True)


parameters = {
    # 'lgb_r__num_leaves': [10, 20, 30],
    # 'lgb_r__learning_rate': [0.05, 0.1, 0.15],
    # 'lgb_r__n_estimators': [50],
    # 'lgb_r__reg_lambda': [10000]  # L2 ê·œì œ ì¶”ê°€
    'lgb_r__random_state': [321]
}

pipe = Pipeline(
    [
        ('pca', PCA(n_components=8)),
        ('lgb_r', LGBMRegressor())
    ]
)

grid_lgb = GridSearchCV(pipe, param_grid=parameters, cv=kfold, scoring='r2')
grid_lgb.fit(X_train, y_train)

# ìµœì ì˜ íŒŒë¼ë¯¸í„°ì™€ ì„±ëŠ¥ ì¶œë ¥
print("Best parameters:", grid_lgb.best_params_)
print("Best cross-validation score: {:.3f}".format(grid_lgb.best_score_))

prediction = grid_lgb.predict(X_test)
get_evaluation_negative(y_test, prediction)

- MSE: 0.3039, RMSE: 0.5513, R2: 0.7087
```

```
import matplotlib.pyplot as plt


r_X_train, v_X_train, r_y_train, v_y_train = \
train_test_split(X_train, y_train, test_size= 0.3, random_state=321)

r_X_train_prediction = grid_lgb.predict(r_X_train)
get_evaluation_negative(r_y_train, r_X_train_prediction)

v_X_train_prediction = grid_lgb.predict(v_X_train)
get_evaluation_negative(v_y_train, v_X_train_prediction)


fig, ax = plt.subplots(1, 2, figsize= (12, 5))

ax[0].scatter(r_y_train, r_X_train_prediction, edgecolors='red', c='red', alpha=0.2)
ax[0].plot([r_y_train.min(), r_y_train.max()], [r_y_train.min(), r_y_train.max()], 'k--')
ax[0].set_title('Train Data Prediction')

ax[1].scatter(v_y_train, v_X_train_prediction, edgecolors='red', c='blue', alpha=0.2)
ax[1].plot([v_y_train.min(), v_y_train.max()], [v_y_train.min(), v_y_train.max()], 'k--')
ax[1].set_title('Validation Data Prediction')
plt.show()
```

```
# í…ŒìŠ¤íŠ¸ ê·¸ë˜í”„ í™•ì¸
import matplotlib.pyplot as plt

prediction = grid_lgb.predict(X_test)
get_evaluation_negative(y_test, prediction)

fig, ax = plt.subplots()
ax.scatter(y_test, prediction, edgecolors='red', c='orange', alpha=0.2)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')
plt.show()
```

</details>

    
<img width="667" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 11 17 10" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/336d165c-1a65-48f9-a570-6e0d03e20ae4">
<img width="369" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 11 17 15" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/98016270-1533-4264-80c6-c67ab56e2079">


<details>
  <summary>Click Graph Code</summary>
    
```
bar ê·¸ë˜í”„ ì²¨ë¶€
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ë°ì´í„° ì¤€ë¹„
data = {
    "Model_Cycle": [
        "Cycle01 Linear", "Cycle01 Poly", "Cycle01 DecisionTree",
        "Cycle01 RandomForest", "Cycle01 GradientBoosting", "Cycle01 XGB", "Cycle01 LGBM",
        "Cycle02 LGBM Train", "Cycle02 LGBM Validation", "Cycle02 LGBM Test",
        "Cycle03 LGBM Train", "Cycle03 LGBM Validation", "Cycle03 LGBM Test"
    ],
    "R2": [
        0.5210, 0.6228, 0.8324,
        0.9118, 0.8079, 0.9196, 0.9003,
        0.7131, 0.7127, 0.7071,
        0.7156, 0.7113, 0.7083
    ]
}

df = pd.DataFrame(data)

# ë§‰ëŒ€ ê·¸ë˜í”„ ì„¤ì •
plt.figure(figsize=(12, 8))  # ê·¸ë˜í”„ í¬ê¸° ì¡°ì ˆ
bar_plot = sns.barplot(x="Model_Cycle", y="R2", data=df, palette="viridis")

# ì‚¬ì´í´ë³„ ìƒ‰ìƒ ì§€ì •
colors = ['red', 'green', 'blue']
cycle_colors = {cycle: colors[i % len(colors)] for i, cycle in enumerate(df['Model_Cycle'].apply(lambda x: x.split()[0]).unique())}

for bar, color in zip(bar_plot.patches, df['Model_Cycle'].apply(lambda x: cycle_colors[x.split()[0]])):
    bar.set_color(color)  # ê° ë§‰ëŒ€ì— ìƒ‰ìƒ ì ìš©

# ê° ë§‰ëŒ€ì— R2 ì ìˆ˜ í…ìŠ¤íŠ¸ ì¶”ê°€
for bar in bar_plot.patches:
    bar_plot.annotate(format(bar.get_height(), '.4f'),
                      (bar.get_x() + bar.get_width() / 2, bar.get_height()),
                      ha='center', va='center',
                      xytext=(0, 9),
                      textcoords='offset points')

# ë ˆì´ë¸” ë° íƒ€ì´í‹€ ì„¤ì •
plt.xlabel("Model and Cycle")
plt.ylabel("R2 Score")
plt.title("R2 Scores Across Different Cycles and Models")
plt.xticks(rotation=45)  # xì¶• ë ˆì´ë¸” íšŒì „
plt.ylim(0.4, 1.0)  # yì¶• ë²”ìœ„ ì¡°ì •
plt.grid(True, linestyle='--', linewidth=0.5, color='gray', axis='y', zorder=0)

# ê·¸ë˜í”„ ë³´ì—¬ì£¼ê¸°
plt.tight_layout()
plt.show()
```

</details>

<img width="726" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/79124254-e101-4534-9d20-7b6e1b645d60">

<hr>

- ì •ë¦¬

    - **ì˜ˆì¸¡ ê°€ëŠ¥í•œ ëª¨ë¸ ê°œë°œ**: ë¶„ì„ì„ í†µí•´ ê°œë°œëœ ëª¨ë¸ì€ ì°¨ì› ì¶•ì†Œ í›„ì—ë„ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©°, ë³‘ì›ì˜ í‡´ì› ê¸ˆì•¡ì„ ì–´ëŠ ì •ë„ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - **ê³¼ì í•© ë¶€ì¬**: ê³¼ì í•©ì´ ê´€ì°°ë˜ì§€ ì•Šì•„, ëª¨ë¸ì— ì¶”ê°€ì ì¸ ê·œì œë¥¼ ì ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ íŠ¸ë¦¬ ê¸°ë°˜ íšŒê·€ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜í™”ëœ ê²°ê³¼ë¥¼ ì œê³µí•˜ê³  ìˆìŒì„ íŒë‹¨í•˜ì˜€ìŠµë‹ˆë‹¤.

- ê²°ë¡ 

    - ë³¸ ë°ì´í„°ì…‹ì€ ë§¤ìš° ë†’ì€ ì‹ ë¢°ì„±ì„ ê°€ì§€ê³  ìˆë‹¤ê³  íŒë‹¨ë˜ë©°, í™˜ìì˜ í‡´ì› ê¸ˆì•¡ì— ëŒ€í•´ ë‹¤ì–‘í•œ ìš”ì†Œë“¤ì´ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì—¬ ì£¼ì—ˆìŠµë‹ˆë‹¤.
    - ë¶„ì„ ê²°ê³¼, ë°ì´í„°ì˜ ë‹¤ì–‘í•œ ìš”ì†Œë“¤ì€ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ë©°, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë†’ì€ ì„±ëŠ¥ì˜ íšŒê·€ ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
    - í˜„ì¬ ëª¨ë¸ì€ ì£¼ë¡œ ì˜ì‚¬ì˜ ë©´í—ˆ ì •ë³´ì— ê¸°ë°˜í•˜ì—¬ êµ¬ì¶•ë˜ì—ˆìœ¼ë‚˜, ì§„ë£Œ ë¶„ê³¼ ê°™ì€ ì¶”ê°€ì ì¸ ì •ë³´ê°€ í¬í•¨ëœë‹¤ë©´ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.
