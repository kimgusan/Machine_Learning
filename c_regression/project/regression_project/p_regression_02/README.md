# â‘  Regression01

## ì£¼ì œ: ğŸ˜Š ì§ì—…ì  ì‚¶ê³¼ ê°œì¸ ìƒí™œ ì „ë°˜ì ì¸ ë§Œì¡± ì ìˆ˜

    (1) ë°ì´í„° ì›ë³¸: ex) https://kaggle.com)

### ëª©ì°¨

1. **ê°€ì„¤ ì„¤ì •**
2. **ë°ì´í„° ë¶„ì„**
3. **ë°ì´í„° ì „ì²˜ë¦¬**
4. **ë°ì´í„° í›ˆë ¨**
    - Cycle (n)ë°˜ë³µ - ê° ì´ë¯¸ì§€ì— ëŒ€í•˜ì—¬ ë§í¬ë¥¼ ê±¸ì–´ì„œ í™•ì¸í•  ê²ƒ.
5. **ê²°ë¡ **

### 1. ê°€ì„¤ ì„¤ì •

#### ê°€ì„¤ 1: ìš”ì†Œë“¤ê³¼ ìì „ê±° ë°°ì¹˜ ê°œìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„

-   **ê°€ì„¤ ë‚´ìš©**  
    ì—¬ëŸ¬ ìš”ì†Œë“¤ì´ ìì „ê±° ëŒ€ì—¬ ìˆ˜ëŸ‰ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.  
    ì´ ìš”ì†Œë“¤ê³¼ ìì „ê±° ë°°ì¹˜ ê°œìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ìš”ì†Œë“¤ì´ ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ íŒŒì•…í•©ë‹ˆë‹¤.

#### ê°€ì„¤ 2: ìì „ê±° ìˆ˜ëŸ‰ ì˜ˆì¸¡ì„ í†µí•œ íš¨ìœ¨ì  ê´€ë¦¬ ê°€ëŠ¥ì„±

-   **ê°€ì„¤ ë‚´ìš©**  
    ì˜ˆì¸¡ ëª¨ë¸ì„ í†µí•´ ê³µìœ í•  ìˆ˜ ìˆëŠ” ìì „ê±°ì˜ ìˆ˜ë¥¼ ì˜ˆì¸¡í•¨ìœ¼ë¡œì¨, ìì „ê±° ê³µìœ  íšŒì‚¬ê°€ ìì „ê±° ìˆ˜ë¥¼ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ê³„íší•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•©ë‹ˆë‹¤.

### 2. ë°ì´í„° ë¶„ì„

#### í†µê³„ì  ë¶„ì„ ë°©ë²•

-   ìˆ˜ì§‘ëœ ë°ì´í„°ì— ëŒ€í•´ ìƒê´€ê´€ê³„ ë¶„ì„, íšŒê·€ ë¶„ì„ ë“±ì„ ì‹¤ì‹œí•˜ì—¬ ê° ìš”ì†Œê°€ ìì „ê±° ë°°ì¹˜ ê°œìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì˜ ì •ë„ì™€ ë°©í–¥ì„ íŒŒì•…í•©ë‹ˆë‹¤.

#### ëª¨ë¸ë§ê³¼ ìµœì í™”

-   ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ ëª¨ë¸ì„ ìƒì„±í•˜ê³ , ì´ ëª¨ë¸ì„ ìµœì í™”í•˜ì—¬ ê°€ì„¤ì˜ íƒ€ë‹¹ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤

<hr>

### 2. ë°ì´í„° ë¶„ì„

```
# ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì œì™¸í•œ ë°ì´í„° í”„ë ˆì„ ìƒì„±
origin_b_df = b_df.copy()
pre_b_df = b_df.copy()
pre_b_df = pre_b_df.drop(labels = ['Date', 'Seasons','Holiday','Functioning Day'], axis=1)

# íƒ€ê²Ÿ ì»¬ëŸ¼ ìœ„ì¹˜ ë³€ê²½
target_column =  pre_b_df.pop('Rented Bike Count')
pre_b_df.loc[:, 'target'] = target_column
pre_b_df

pre_b_df.isna().sum()
pre_b_df.duplicated().sum()

# ìƒê´€ê´€ê³„ í™•ì¸
pre_b_df.corr()['target'].sort_values(ascending=False)[1:]

- ì‹œê°í™” ì´ë¯¸ì§€ ì²¨ë¶€

- OLS ì§€í‘œ ì²¨ë¶€


```

### 3. ë°ì´í„° ì „ì²˜ë¦¬

```
# ì¤‘ë³µê°’ í™•ì¸/
pre_l_df = pre_l_df.drop_duplicates().reset_index(drop=True)

# ìƒê´€ê´€ê³„ í™•ì¸
pre_l_df.corr()['WORK_LIFE_BALANCE_SCORE'].sort_values(ascending=False)[1:]

- ì´ë¯¸ì§€ íšŒê·€ì„  ë„£ê¸°
- corr ì´ë¯¸ì§€ ë„£ê¸°
- ìƒê´€ê´€ê³„ ì´ë¯¸ì§€ ë„—ê¸°
- ì„ í˜• ì´ë¯¸ì§€ ë„£ê¸°
- ì–‘ì˜ ìƒê´€ê´€ê³„ ìŒì˜ ìƒê´€ê´€ê³„ ë„£ê¸°
- ë‹¤ì¤‘ê³µì‚°ì„± ì§€í‘œ ë„£ê¸°
- OLS ì§€í‘œ ë„£ê¸°

```

### 4. ë°ì´í„° í›ˆë ¨

-   Cycle01
    1. íƒ€ê²Ÿí˜• ë°ì´í„°ë¥¼ ì œì™¸í•œ ëª¨ë¸ í›ˆë ¨ ì§„í–‰, r2 score í™•ì¸

```
# ì„ í˜• ë°ì´í„° í™•ì¸
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

features, targets = pre_b_df.iloc[:, :-1], pre_b_df.iloc[:, -1]

X_train, X_test, y_train, y_test = \
train_test_split(features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 208294.7827, RMSE: 456.3932, R2: 0.4893
```

```
# ë¹„ì„ í˜• ë°ì´í„° í™•ì¸
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly_features = PolynomialFeatures(degree=3).fit_transform(features)

X_train, X_test, y_train, y_test =\
train_test_split(poly_features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 140395.6319, RMSE: 374.6941, R2: 0.6558
```

```
# í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ê·¸ë˜í”„ í™•ì¸
import matplotlib.pyplot as plt


r_X_train, v_X_train, r_y_train, v_y_train = \
train_test_split(X_train, y_train, test_size= 0.3, random_state=321)

r_X_train_prediction = l_r.predict(r_X_train)
get_evaluation_negative(r_y_train, r_X_train_prediction)

v_X_train_prediction = l_r.predict(v_X_train)
get_evaluation_negative(v_y_train, v_X_train_prediction)


fig, ax = plt.subplots(1, 2, figsize= (12, 5))

ax[0].scatter(r_y_train, r_X_train_prediction, edgecolors='red', c='red', alpha=0.2)
ax[0].plot([r_y_train.min(), r_y_train.max()], [r_y_train.min(), r_y_train.max()], 'k--')
ax[0].set_title('Train Data Prediction')

ax[1].scatter(v_y_train, v_X_train_prediction, edgecolors='red', c='blue', alpha=0.2)
ax[1].plot([v_y_train.min(), v_y_train.max()], [v_y_train.min(), v_y_train.max()], 'k--')
ax[1].set_title('Validation Data Prediction')
plt.show()


    MSE: 128546.7844, RMSE: 358.5342, R2: 0.6928
    MSE: 132403.7434, RMSE: 363.8733, R2: 0.6823

- í›ˆë ¨ ë°ì´í„° ê²€ì¦ ê·¸ë˜í”„ ì²¨ë¶€

```

```
- test ê²€ì‚¬ í•¨ìˆ˜.

import matplotlib.pyplot as plt

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

fig, ax = plt.subplots()
ax.scatter(y_test, prediction, edgecolors='red', c='orange', alpha=0.2)
ax.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--')
plt.show()
- test ê²€ì‚¬ ì²¨ë¶€
```

```
# C01
- ë¹„ì„ í˜•í›ˆë ¨ì—ì„œ ë” ë†’ì€ ì ìˆ˜ë¥¼ ë³´ì´ê³  ìˆê¸° ë•Œë¬¸ì— ë¹„ì„ í˜• ë°ì´í„° í™•ì¸.
- val data ì™€ train ë°ì´í„°ì—ì„œ í° ì°¨ì´ëŠ” ë³´ì´ì§€ ì•Šê¸° ë•Œë¬¸ì— ë³„ë„ì˜ ê³¼ì í•©ì¸ì§€ íŒë‹¨ ë¶ˆê°€
- ì´ìƒì¹˜ê°€ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ í™•ì¸
```

```
    # í›ˆë ¨ ê²°ê³¼ Cycle_01
    prediction = l_r.predict(X_test)
    get_evaluation(y_test, prediction)
```

-   Cycle02
    -   ê¸°ì¡´ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì¶”ê°€í•œ í›„ í›ˆë ¨ ì§„í–‰

```
# ë²”ì£¼í˜• ë°ì´í„° ë ˆì´ë¸”ì¸ì½”ë”©
from sklearn.preprocessing import LabelEncoder

columns = ['Seasons', 'Holiday']
label_encoders = {}

for column in columns:
    encoder = LabelEncoder()
    result = encoder.fit_transform(origin_b_df[column])
    origin_b_df[column] = result
    label_encoders[column] = encoder.classes_

label_encoders

- OLS ì§€í‘œ ì²¨ë¶€
```

```
# Lineare Regresssion í›ˆë ¨
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

features, targets = origin_b_df.iloc[:, :-1], origin_b_df.iloc[:, -1]

X_train, X_test, y_train, y_test = \
train_test_split(features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 203459.3725, RMSE: 451.0647, R2: 0.5012

```

```
# ë¹„ì„ í˜•ëª¨ë¸ í›ˆë ¨
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly_features = PolynomialFeatures(degree=2).fit_transform(features)

X_train, X_test, y_train, y_test =\
train_test_split(poly_features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 162936.3859, RMSE: 403.6538, R2: 0.6005
```

```
# íŠ¸ë¦¬ ëª¨ë¸ í›ˆë ¨
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

features, targets = origin_b_df.iloc[:, :-1], origin_b_df.iloc[:, -1]

X_train, X_test, y_train, y_test =\
train_test_split(features, targets, test_size=0.2, random_state=321)

dt_r = DecisionTreeRegressor(random_state=321)
rd_r = RandomForestRegressor(random_state=321)
gb_r = GradientBoostingRegressor(random_state=321)
xgb_r = XGBRegressor(random_state=321)
lgb_r = LGBMRegressor(random_state=321)

models = [dt_r, rd_r, gb_r, xgb_r, lgb_r]

for model in models:
    model.fit(X_train, y_train)
    prediction = model.predict(X_test)
    print(model.__class__.__name__)
    get_evaluation_negative(y_test, prediction)

DecisionTreeRegressor
MSE: 180518.2426, RMSE: 424.8744, R2: 0.5574
RandomForestRegressor
MSE: 85586.0729, RMSE: 292.5510, R2: 0.7902
GradientBoostingRegressor
MSE: 91201.5446, RMSE: 301.9959, R2: 0.7764
XGBRegressor
MSE: 86721.8676, RMSE: 294.4858, R2: 0.7874
LGBMRegressor
MSE: 81825.2797, RMSE: 286.0512, R2: 0.7994
```

```
ë²”ì£¼í˜• ë°ì´í„° ì¶”ê°€ ì‹œ ë¹„ì„ í˜• ë°ì´í„° ìˆ˜ì¹˜ê°€ ë†’ì•„ì§€ëŠ” ë¶€ë¶„ í™•ì¸
íŠ¸ë¦¬ê¸°ë°˜ì˜ ëª¨ë¸ì—ì„œ í›ˆë ¨ì´ ë” ì˜ë˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì—¬ íŠ¸ë¦¬ê¸°ë°˜ì˜ ëª¨ë¸ ì„ íƒ
```

-   Cycle03
    -   ì´ìƒì¹˜ë¥¼ í™•ì¸í•˜ì˜€ê¸° ë•Œë¬¸ì— ì´ìƒì¹˜ ì œê±° í›„ í›ˆë ¨ ì§„í–‰

```
# ì´ìƒì¹˜ ì œê±°ë¥¼ ìœ„í•œ í‘œì¤€í™” ì‘ì—…
from sklearn.preprocessing import StandardScaler

std = StandardScaler()
result = std.fit_transform(origin_b_df)
std_origin_b_df = pd.DataFrame(result, columns = origin_b_df.columns)
std_origin_b_df

# ì´ìƒì¹˜ í™•ì¸ ë° ì œê±°
condition = True
error_count = []

for column in std_origin_b_df.columns:
    if std_origin_b_df[column].between(-1.96, 1.96) is True:
        error_count.append(std_origin_b_df[column].between(-1.96, 1.96).count())
    condition &= std_origin_b_df[column].between(-1.96, 1.96)

std_origin_b_df = std_origin_b_df[condition]
std_origin_b_df

# ì´ìƒì¹˜ ì œê±°í•œ ë°ì´í„°ë¥¼ ì¸ë±ìŠ¤ ë²ˆí˜¸ì— ë§ê²Œ ê°€ì ¸ì˜¤ê¸°
origin_b_df = origin_b_df.iloc[std_origin_b_df.index].reset_index(drop=True)
origin_b_df
```

```
# ì„ í˜• íšŒê·€ ëª¨ë¸ í›ˆë ¨
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

features, targets = origin_b_df.iloc[:, :-1], origin_b_df.iloc[:, -1]

X_train, X_test, y_train, y_test = \
train_test_split(features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 141044.1497, RMSE: 375.5585, R2: 0.4673
```

```
# ë¹„ì„ í˜• ëª¨ë¸ í™•ì¸
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly_features = PolynomialFeatures(degree=2).fit_transform(features)

X_train, X_test, y_train, y_test =\
train_test_split(poly_features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 125052.8676, RMSE: 353.6281, R2: 0.5277
```

```
# íŠ¸ë¦¬ ëª¨ë¸ í™•ì¸
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

features, targets = origin_b_df.iloc[:, :-1], origin_b_df.iloc[:, -1]

X_train, X_test, y_train, y_test =\
train_test_split(features, targets, test_size=0.2, random_state=321)

dt_r = DecisionTreeRegressor(random_state=321)
rd_r = RandomForestRegressor(random_state=321)
gb_r = GradientBoostingRegressor(random_state=321)
xgb_r = XGBRegressor(random_state=321)
lgb_r = LGBMRegressor(random_state=321)

models = [dt_r, rd_r, gb_r, xgb_r, lgb_r]

for model in models:
    model.fit(X_train, y_train)
    prediction = model.predict(X_test)
    print(model.__class__.__name__)
    get_evaluation_negative(y_test, prediction)

DecisionTreeRegressor
MSE: 124514.8276, RMSE: 352.8666, R2: 0.5297
RandomForestRegressor
MSE: 75376.0106, RMSE: 274.5469, R2: 0.7153
GradientBoostingRegressor
MSE: 83471.2015, RMSE: 288.9138, R2: 0.6847
XGBRegressor
MSE: 81891.1387, RMSE: 286.1663, R2: 0.6907
LGBMRegressor
MSE: 76253.2006, RMSE: 276.1398, R2: 0.7120
```

```
- íŠ¸ë¦¬ ëª¨ë¸ì— ëŒ€í˜„ validation train ì´ë¯¸ì§€ ì²¨ë¶€
- test ë°ì´í„°ì— ëŒ€í•œ ì´ë¯¸ì§€ ì²¨ë¶€
```

-   Cycle04
    -   ê³¼ì í•©ì˜ ì •ë„ë¥¼ íŒë‹¨íˆê¸° ìœ„í•´ êµì°¨ê²€ì¦ì„ ì§„í–‰
    -   ì´ì „ ê²€ì‚¬ì—ì„œ ë°ì´í„°ì˜ ì •ë„ë¥¼ ë´¤ì„ ë•Œ ê³¼ì í•©ì´ ìˆì„ ìˆ˜ ìˆë‹¤ëŠ” íŒë‹¤.

```
from sklearn.model_selection import cross_val_score, KFold

features, targets = origin_b_df.iloc[:,:-1], origin_b_df.iloc[:,-1]

kf = KFold(n_splits=10, random_state=124, shuffle=True)
scores = cross_val_score(lgb_r, features, targets , cv=kf)
scores

- scoreì— ëŒ€í•œ ì´ë¯¸ì§€ ì²¨ë¶€í•˜ê¸°
```

```
# êµì°¨ê²€ì¦ ë° l2 ê·œì œ
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.pipeline import Pipeline


features, targets = origin_b_df.iloc[:,:-1], origin_b_df.iloc[:,-1]

X_train, X_test, y_train, y_test = \
train_test_split(features,targets, test_size=0.2, random_state=321)

kfold = KFold(n_splits=10, random_state=124, shuffle=True)

lgb_r = LGBMRegressor()

parameters = {
    'random_state': [321],
    # 'min_gain_to_split' : [0.01],
    # 'max_depth' : [10],
    # 'num_leaves' : [31],
    # 'reg_lambda': [100],
    'verbose': [-1]
}

g_lgb_r = GridSearchCV(lgb_r, param_grid=parameters, cv=kfold, scoring='neg_mean_squared_error')
g_lgb_r.fit(X_train, y_train)

# ìµœì ì˜ íŒŒë¼ë¯¸í„°ì™€ ì„±ëŠ¥ ì¶œë ¥
print("Best parameters:", g_lgb_r.best_params_)
print("Best cross-validation score: {:.3f}".format(-g_lgb_r.best_score_))

prediction = g_lgb_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 76253.2006, RMSE: 276.1398, R2: 0.7120

- train í•˜ê³  validationê·¸ë˜í”„ ì²¨ë¶€í•˜ê¸°
- test ë°ì´í„° ì²˜ë­…í•˜ê¸°
```

-   Cycle05
    -   ëª¨ë¸ì˜ ì¼ë°˜í™”ë¥¼ ìœ„í•´ ë‹¤ì¤‘ê³µì‚°ì„±ê³¼ ìƒê´€ê´€ê³„í™•ì¸ã„¹í›„ ë¶ˆí•„ìš” featrue ì œê±°.

```
- OLSì§€í‘œ í™•ì¸
- ìƒê´€ê´€ê³„ í™•ì¸
origin_b_df = origin_b_df.drop(labels = ['Holiday'], axis = 1)
- ë°ì´í„° ì „ì²˜ë¦¬ í›„ OLS ì§€í‘œ í™•ì¸
origin_b_df = origin_b_df.drop(labels = ['Humidity(%)', 'Wind speed (m/s)', 'Solar Radiation (MJ/m2)'], axis = 1)
- ë°ì´í„° ì „ì²˜ë¦¬ í›„ OLS ì§€í‘œ í™•ì¸


from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.pipeline import Pipeline


features, targets = origin_b_df.iloc[:,:-1], origin_b_df.iloc[:,-1]

X_train, X_test, y_train, y_test = \
train_test_split(features,targets, test_size=0.2, random_state=321)

kfold = KFold(n_splits=10, random_state=124, shuffle=True)

lgb_r = LGBMRegressor()

parameters = {
    'random_state': [321],
    # 'reg_lambda': [100],
    'verbose': [-1]
}

g_lgb_r = GridSearchCV(lgb_r, param_grid=parameters, cv=kfold, scoring='neg_mean_squared_error')
g_lgb_r.fit(X_train, y_train)

# ìµœì ì˜ íŒŒë¼ë¯¸í„°ì™€ ì„±ëŠ¥ ì¶œë ¥
print("Best parameters:", g_lgb_r.best_params_)
print("Best cross-validation score: {:.3f}".format(-g_lgb_r.best_score_))

prediction = g_lgb_r.predict(X_test)
get_evaluation_negative(y_test, prediction)


MSE: 73783.6514, RMSE: 271.6315, R2: 0.7213

- train, validation data ê·¸ë˜í”„ í™•ì¸
- test ë°ì´í„° í™•ì¸
```

-   Cycle06
    -   ê° ìˆ˜ì¹˜í˜• ë°ì´í„°ì— ëŒ€í•˜ì—¬ powertransform ì„ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜í™” ê°•í™”
    -   Ridge ê·œì œë¥¼ í†µí•´ ê³¼ì í•¨ì„ ì¶”ê°€ì ìœ¼ë¡œ ë°©ì§€

```
from sklearn.preprocessing import PowerTransformer

columns = pre_b_df.iloc[:, :-1].columns
power_b_df = pre_b_df.copy()

for column in columns:
    ptf = PowerTransformer(standardize=False)
    result = ptf.fit_transform(pre_b_df[[column]])
    power_b_df[column] = result

power_b_df

- OLS ì§€í‘œ ì²¨ë¶€

- ë°ì´í„° ì „ì²˜ë¦¬
power_b_df = power_b_df.drop(labels = ['Wind speed (m/s)'], axis = 1)

from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.pipeline import Pipeline


features, targets = power_b_df.iloc[:,:-1], power_b_df.iloc[:,-1]

X_train, X_test, y_train, y_test = \
train_test_split(features,targets, test_size=0.2, random_state=321)

kfold = KFold(n_splits=10, random_state=124, shuffle=True)

lgb_r = LGBMRegressor()

parameters = {
    'random_state': [321],
    'reg_lambda': [100],
    'verbose': [-1]
}

g_lgb_r = GridSearchCV(lgb_r, param_grid=parameters, cv=kfold, scoring='neg_mean_squared_error')
g_lgb_r.fit(X_train, y_train)

# ìµœì ì˜ íŒŒë¼ë¯¸í„°ì™€ ì„±ëŠ¥ ì¶œë ¥
print("Best parameters:", g_lgb_r.best_params_)
print("Best cross-validation score: {:.3f}".format(-g_lgb_r.best_score_))

prediction = g_lgb_r.predict(X_test)
get_evaluation_negative(y_test, prediction)


MSE: 88352.2831, RMSE: 297.2411, R2: 0.7834


- train, validation ê·¸ë˜í”„ ì²˜ë¶€
- test ë°ì´í„° ê·¸ë˜í”„ ì²¨ë¶€
```

-   Cycle07
    -   ìˆ˜ì¹˜í­ì„ ë” ì¢í ìˆ˜ ìˆëŠ” ì§€ ì¶”ê°€ ì „ì²˜ë¦¬ ì§„í–‰
    -   ë¶ˆí•„ìš” feature ì‚­ì œ.

```
    - ìƒê´€ê´€ê³„ ìˆ˜ì¹˜ ê·¸ë˜í”„ í™•ì¸
    - OLS ì§€í‘œ í™•ì¸
    - ë‹¤ì¤‘ê³µì„ ì„± ì§€í‘œ í™•ì¸

    power_b_df = power_b_df.drop(labels = ['Snowfall (cm)'], axis=1)
    - OLS ì§€í‘œ í™•ì¸

    from lightgbm import LGBMRegressor
    from sklearn.model_selection import train_test_split, GridSearchCV, KFold
    from sklearn.pipeline import Pipeline


    features, targets = power_b_df.iloc[:,:-1], power_b_df.iloc[:,-1]

    X_train, X_test, y_train, y_test = \
    train_test_split(features,targets, test_size=0.2, random_state=321)

    kfold = KFold(n_splits=10, random_state=124, shuffle=True)

    lgb_r = LGBMRegressor()

    parameters = {
        'random_state': [321],
        'reg_lambda': [100],
        'verbose': [-1]
    }

    g_lgb_r = GridSearchCV(lgb_r, param_grid=parameters, cv=kfold, scoring='neg_mean_squared_error')
    g_lgb_r.fit(X_train, y_train)

    # ìµœì ì˜ íŒŒë¼ë¯¸í„°ì™€ ì„±ëŠ¥ ì¶œë ¥
    print("Best parameters:", g_lgb_r.best_params_)
    print("Best cross-validation score: {:.3f}".format(-g_lgb_r.best_score_))

    prediction = g_lgb_r.predict(X_test)
    get_evaluation_negative(y_test, prediction)

    MSE: 88007.3417, RMSE: 296.6603, R2: 0.7842




```

import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

X_train, X_test, y_train, y_test =\
train_test_split(features, targets, test_size=0.2, random_state=321)

r_X_train, v_X_train, r_y_train, v_y_train = \
train_test_split(X_train, y_train, test_size= 0.3, random_state=321)

# ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡

g_lgb_r.fit(r_X_train, r_y_train)
prediction_r_train = g_lgb_r.predict(r_X_train)
prediction_v_train = g_lgb_r.predict(v_X_train)

# í‰ê°€

get_evaluation_negative(r_y_train, prediction_r_train)
get_evaluation_negative(v_y_train, prediction_v_train)

# ì‹œê°í™”

fig, ax = plt.subplots(1, 2, figsize=(12, 5))
ax[0].scatter(r_y_train, prediction_r_train, edgecolors='red', c='red', alpha=0.2)
ax[0].plot([r_y_train.min(), r_y_train.max()], [r_y_train.min(), r_y_train.max()], 'k--')
ax[0].set_title('Train Data Prediction')

ax[1].scatter(v_y_train, prediction_v_train, edgecolors='red', c='blue', alpha=0.2)
ax[1].plot([v_y_train.min(), v_y_train.max()], [v_y_train.min(), v_y_train.max()], 'k--')
ax[1].set_title('Validation Data Prediction')
plt.show()

MSE: 69007.0353, RMSE: 262.6919, R2: 0.8351
MSE: 99120.6772, RMSE: 314.8344, R2: 0.7621

-   train, validation ê·¸ë˜í”„ ì²¨ë¶€

import matplotlib.pyplot as plt

prediction = g_lgb_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

fig, ax = plt.subplots()
ax.scatter(y_test, prediction, edgecolors='red', c='orange', alpha=0.2)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')
plt.show()
ìµœì¢… ê° R2 Scoreì— ëŒ€í•œ ë§‰ëŒ€ê·¸ë˜í”„ ì  ì²¨ë¶€ í•„ìš”.

```
- ì‹œê°í™” ê·¸ë˜í”„ ì‚¬ìš©í•˜ì—¬ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…
- ì‹œê°í™” ê·¸ë˜í”„ë¥¼ íŒ€ì›ì¤‘ ë³¸ ì‚¬ëŒì´ ì—†ì–´ í”¼íŠ¸ë°± ë¶ˆê°€
```

-   ê²°ê³¼

        - ê°€ì„¤

    ëª©ì : ìì „ê±° ëŒ€ì—¬ ì„œë¹„ìŠ¤ì—ì„œ ìì „ê±° ë°°ì¹˜ ê°œìˆ˜ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì£¼ìš” ìš”ì†Œë“¤ì„ ì‹ë³„í•˜ê³  ì´í•´í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

-   ì£¼ìš” ë‚´ìš©:  
    ì—¬ëŸ¬ ìš”ì†Œë“¤ì´ ìì „ê±° ëŒ€ì—¬ ìˆ˜ëŸ‰ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    ì´ ìš”ì†Œë“¤ê³¼ ìì „ê±° ë°°ì¹˜ ê°œìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ìš”ì†Œë“¤ì´ ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ íŒŒì•…í•©ë‹ˆë‹¤.
    ì˜ˆì¸¡ ëª¨ë¸ì„ í†µí•´ ê³µìœ í•  ìˆ˜ ìˆëŠ” ìì „ê±°ì˜ ìˆ˜ë¥¼ ì˜ˆì¸¡í•¨ìœ¼ë¡œì¨, ìì „ê±° ê³µìœ  íšŒì‚¬ê°€ ìì „ê±° ìˆ˜ë¥¼ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ê³„íší•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•©ë‹ˆë‹¤.

-   ê²°ë¡ 

-   ê³¼ì í•© ë¬¸ì œ ì‹ë³„: Validation ê·¸ë˜í”„ë¥¼ í†µí•´ Train ë°ì´í„°ì™€ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì— ê³¼ì í•©ì´ ìˆìŒì„ íŒë‹¨í–ˆìŠµë‹ˆë‹¤.
-   ëª¨ë¸ ì¼ë°˜í™” ê°œì„ : ë‹¤ì¤‘ê³µì„ ì„± ê²€ì‚¬, Power Transform ì ìš©, ìƒê´€ê´€ê³„ ë¶„ì„ì„ í†µí•´ ëª¨ë¸ì˜ ì¼ë°˜í™”ë¥¼ ë„ëª¨í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶„ì„ì„ í†µí•´ ëª¨ë¸ì´ ë”ìš± ê°•ê±´í•´ì§ˆ ìˆ˜ ìˆë„ë¡ ì¡°ì¹˜ë¥¼ ì·¨í–ˆìŠµë‹ˆë‹¤.
    L2 ê·œì œ ì ìš©: êµì°¨ ê²€ì¦ì„ í†µí•´ L2 ê·œì œë¥¼ ì ìš©í•¨ìœ¼ë¡œì¨ ê³¼ì í•©ì„ ë°©ì§€í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê²€ì¦ ë°ì´í„°ì—ì„œì˜ ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.

-   ì¢…í•©ì ì¸ ê´€ì 
    ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ìì „ê±° ëŒ€ì—¬ ìˆ˜ëŸ‰ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì£¼ìš” ìš”ì†Œë“¤ì„ íŒŒì•…í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íš¨ê³¼ì ì¸ ìì „ê±° ë°°ì¹˜ ì „ëµì„ ìˆ˜ë¦½í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ ê³¼ì í•© ë¬¸ì œë¥¼ ì‹ë³„í•˜ê³  ì´ë¥¼ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì ìš©í•¨ìœ¼ë¡œì¨, ëª¨ë¸ì˜ ì‹ ë¢°ì„±ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. ì´ ê²°ê³¼ëŠ” ìì „ê±° ê³µìœ  íšŒì‚¬ì—ê²Œ ìì „ê±° ë°°ì¹˜ ê³„íšì„ ë” ì˜ ìˆ˜ë¦½í•˜ê³ , ê³ ê° ìˆ˜ìš”ë¥¼ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ì¶©ì¡±ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.
