# â‘¡ Regression02 (ë¹„ì„ í˜•)

## ğŸš´ ì£¼ì œ: ì„œìš¸ì‹œ ìì „ê±° ê³µìœ  ê°€ëŠ¥ ìˆ˜ ì˜ˆì¸¡.

    (1) ë°ì´í„° ì›ë³¸: ex) https://kaggle.com)

### ëª©ì°¨

1. **ê°€ì„¤ ì„¤ì •**
2. **ë°ì´í„° ë¶„ì„**
3. **ë°ì´í„° ì „ì²˜ë¦¬**
4. **ë°ì´í„° í›ˆë ¨**
    <details>
        <summary>Cycle</summary>   
        <ul style='list-style-type: none;'>
            <li><a href="#cycle01">Cycle01(íƒ€ê²Ÿí˜• ë°ì´í„°ë¥¼ ì œì™¸í•œ ëª¨ë¸ í›ˆë ¨ ì§„í–‰)</a></li>
            <li><a href='#cycle02'>Cycle02(ê¸°ì¡´ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì¶”ê°€í•œ í›„ í›ˆë ¨ ì§„í–‰)</a></li>
            <li><a href='#cycle03'>Cycle03(ì´ìƒì¹˜ ì œê±° í›„ í›ˆë ¨ ì§„í–‰)</a></li>
            <li><a href='#cycle04'>Cycle04(ê³¼ì í•©ì˜ ì •ë„ë¥¼ íŒë‹¨íˆê¸° ìœ„í•´ êµì°¨ê²€ì¦ì„ ì§„í–‰)</a></li>
            <li><a href='#cycle05'>Cycle05(ë‹¤ì¤‘ê³µì„ ì„±ê³¼ ìƒê´€ê´€ê³„í™•ì¸ í›„ ë¶ˆí•„ìš” featrue ì œê±°)</a></li>
            <li><a href='#cycle06'>Cycle06(powertransform ì¼ë°”í™” ë° L2 ê·œì œ ì‚¬ìš©í•˜ì—¬ ê³¼ì í•© ë°©ì§€ í›ˆë ¨ ì§„í–‰)</a></li>
            <li><a href='#cycle07'>Cycle07(ë‹¤ì¤‘ê³µì„ ì„±ê³¼ ìƒê´€ê´€ê³„í™•ì¸ í›„ ë¶ˆí•„ìš” featrue ì œê±°)</a></li>
        </ul>
   </details>
5. **ê²°ë¡ **

<hr>

### 1. ê°€ì„¤ ì„¤ì •

#### ê°€ì„¤ 1: ìš”ì†Œë“¤ê³¼ ìì „ê±° ë°°ì¹˜ ê°œìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„

-   **ê°€ì„¤ ë‚´ìš©**  
    ì—¬ëŸ¬ ìš”ì†Œë“¤ì´ ìì „ê±° ëŒ€ì—¬ ë°°ì¹˜ ìˆ˜ëŸ‰ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.  
    ì´ ìš”ì†Œë“¤ê³¼ ìì „ê±° ë°°ì¹˜ ê°œìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ í•´ë‹¹ ìš”ì†Œë“¤ì´ ì–´ëŠ ì •ë„ì˜ ìì „ê±° ë°°ì¹˜ ëª¨ë¸ì„ ê´€ë¦¬ í•  ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.

#### ê°€ì„¤ 2: ìì „ê±° ìˆ˜ëŸ‰ ì˜ˆì¸¡ì„ í†µí•œ íš¨ìœ¨ì  ê´€ë¦¬ ê°€ëŠ¥ì„±

-   **ê°€ì„¤ ë‚´ìš©**  
    ì˜ˆì¸¡ ëª¨ë¸ì„ í†µí•´ ê³µìœ í•  ìˆ˜ ìˆëŠ” ìì „ê±°ì˜ ìˆ˜ë¥¼ ì˜ˆì¸¡í•¨ìœ¼ë¡œì¨, ìì „ê±° ê³µìœ  íšŒì‚¬ê°€ ìì „ê±° ìˆ˜ë¥¼ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ê³„íší•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•©ë‹ˆë‹¤.

### 2. ë°ì´í„° ë¶„ì„

#### í†µê³„ì  ë¶„ì„ ë°©ë²•

-   ìˆ˜ì§‘ëœ ë°ì´í„°ì— ëŒ€í•´ ìƒê´€ê´€ê³„ ë¶„ì„, íšŒê·€ ë¶„ì„ ë“±ì„ ì‹¤ì‹œí•˜ì—¬ ê° ìš”ì†Œê°€ ìì „ê±° ë°°ì¹˜ ê°œìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì˜ ì •ë„ì™€ ë°©í–¥ì„ íŒŒì•…í•©ë‹ˆë‹¤.

#### ëª¨ë¸ë§ê³¼ ìµœì í™”

-   ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ ëª¨ë¸ì„ ìƒì„±í•˜ê³ , ì´ ëª¨ë¸ì˜ ì¼ë°˜í™”ë¥¼ í†µí•´ ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

<hr>

### 2. ë°ì´í„° ë¶„ì„

```
import pandas as pd

b_df = pd.read_csv('../../datasets/p_bike.csv')
b_df

origin_b_df = b_df.copy()
```
<hr>

### 3. ë°ì´í„° ì „ì²˜ë¦¬

```
# ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì œì™¸í•œ ë°ì´í„° í”„ë ˆì„ ìƒì„±
pre_b_df = b_df.copy()
pre_b_df = pre_b_df.drop(labels = ['Date', 'Seasons','Holiday','Functioning Day'], axis=1)

# íƒ€ê²Ÿ ì»¬ëŸ¼ ìœ„ì¹˜ ë³€ê²½
target_column =  pre_b_df.pop('Rented Bike Count')
pre_b_df.loc[:, 'target'] = target_column
pre_b_df

pre_b_df.isna().sum()
pre_b_df.duplicated().sum()

# ìƒê´€ê´€ê³„ í™•ì¸
pre_b_df.corr()['target'].sort_values(ascending=False)[1:]

# ì¤‘ë³µê°’ í™•ì¸/
pre_l_df = pre_l_df.drop_duplicates().reset_index(drop=True)

# ìƒê´€ê´€ê³„ í™•ì¸
pre_l_df.corr()['WORK_LIFE_BALANCE_SCORE'].sort_values(ascending=False)[1:]
```
<img width="451" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 37 24" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/1c4f5394-80ed-4d86-b741-e1bd29a331af">
<img width="493" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 37 32" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/499ce314-f9a1-4059-87f0-f0bf0443e7c7">



<hr>

### 4. ë°ì´í„° í›ˆë ¨

<h2 id="cycle01">Cycle01</h2>
<p>1. íƒ€ê²Ÿí˜• ë°ì´í„°ë¥¼ ì œì™¸í•œ ëª¨ë¸ í›ˆë ¨ ì§„í–‰, r2 score í™•ì¸</p>

```
# ì„ í˜• ë°ì´í„° í™•ì¸
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

features, targets = pre_b_df.iloc[:, :-1], pre_b_df.iloc[:, -1]

X_train, X_test, y_train, y_test = \
train_test_split(features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 208294.7827, RMSE: 456.3932, R2: 0.4893
```
<img width="243" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 39 21" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/cb7a8afc-a061-4bf5-85a3-b9b7c64b67d9">

```
# ë¹„ì„ í˜• ë°ì´í„° í™•ì¸
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly_features = PolynomialFeatures(degree=3).fit_transform(features)

X_train, X_test, y_train, y_test =\
train_test_split(poly_features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 140395.6319, RMSE: 374.6941, R2: 0.6558
```
<img width="240" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 39 25" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/2ea6d90c-382e-4e93-99c6-73d1c04ad3ca">

```
# í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ê·¸ë˜í”„ í™•ì¸
import matplotlib.pyplot as plt


r_X_train, v_X_train, r_y_train, v_y_train = \
train_test_split(X_train, y_train, test_size= 0.3, random_state=321)

r_X_train_prediction = l_r.predict(r_X_train)
get_evaluation_negative(r_y_train, r_X_train_prediction)

v_X_train_prediction = l_r.predict(v_X_train)
get_evaluation_negative(v_y_train, v_X_train_prediction)


fig, ax = plt.subplots(1, 2, figsize= (12, 5))

ax[0].scatter(r_y_train, r_X_train_prediction, edgecolors='red', c='red', alpha=0.2)
ax[0].plot([r_y_train.min(), r_y_train.max()], [r_y_train.min(), r_y_train.max()], 'k--')
ax[0].set_title('Train Data Prediction')

ax[1].scatter(v_y_train, v_X_train_prediction, edgecolors='red', c='blue', alpha=0.2)
ax[1].plot([v_y_train.min(), v_y_train.max()], [v_y_train.min(), v_y_train.max()], 'k--')
ax[1].set_title('Validation Data Prediction')
plt.show()
```

```
- test ê²€ì‚¬ í•¨ìˆ˜.

import matplotlib.pyplot as plt

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

fig, ax = plt.subplots()
ax.scatter(y_test, prediction, edgecolors='red', c='orange', alpha=0.2)
ax.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--')
plt.show()
```
<img width="675" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 39 51" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/5e3f3d3f-083a-4df9-9175-ae07a2e7b8ad">
<img width="391" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 40 17" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/4d1b89d9-d4d6-4994-b20e-7347e8739873">



```
# C01
- ë¹„ì„ í˜•í›ˆë ¨ì—ì„œ ë” ë†’ì€ ì ìˆ˜ë¥¼ ë³´ì´ê³  ìˆê¸° ë•Œë¬¸ì— ë¹„ì„ í˜• ë°ì´í„° í™•ì¸.
- val data ì™€ train ë°ì´í„°ì—ì„œ í° ì°¨ì´ëŠ” ë³´ì´ì§€ ì•Šê¸° ë•Œë¬¸ì— ë³„ë„ì˜ ê³¼ì í•©ì¸ì§€ íŒë‹¨ ë¶ˆê°€
- ì´ìƒì¹˜ê°€ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ í™•ì¸
```

<h2 id="cycle02">Cycle02</h2>
<p>ê¸°ì¡´ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì¶”ê°€í•œ í›„ í›ˆë ¨ ì§„í–‰</p>

```
# ë²”ì£¼í˜• ë°ì´í„° ë ˆì´ë¸”ì¸ì½”ë”©
from sklearn.preprocessing import LabelEncoder

columns = ['Seasons', 'Holiday']
label_encoders = {}

for column in columns:
    encoder = LabelEncoder()
    result = encoder.fit_transform(origin_b_df[column])
    origin_b_df[column] = result
    label_encoders[column] = encoder.classes_

label_encoders
```

```
# ìƒê´€ê´€ê³„ í™•ì¸
origin_b_df.corr()['target'].sort_values(ascending=False)[1:]
```
<img width="195" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 41 30" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/034ae726-9bab-4543-8efa-1566b90f1017">
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 42 08" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/520dd237-d6db-4507-a723-14c9a6f8704c">


```
# Lineare Regresssion í›ˆë ¨
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

features, targets = origin_b_df.iloc[:, :-1], origin_b_df.iloc[:, -1]

X_train, X_test, y_train, y_test = \
train_test_split(features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 203459.3725, RMSE: 451.0647, R2: 0.5012
```

<img width="243" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 43 08" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/3feb2402-e2a8-4480-adfa-3ec5851c589a">

```
# ë¹„ì„ í˜•ëª¨ë¸ í›ˆë ¨
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly_features = PolynomialFeatures(degree=2).fit_transform(features)

X_train, X_test, y_train, y_test =\
train_test_split(poly_features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 162936.3859, RMSE: 403.6538, R2: 0.6005
```

<img width="245" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 42 39" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/115bf70e-b11d-41cd-b6b0-4705d561c11a">

```
# íŠ¸ë¦¬ ëª¨ë¸ í›ˆë ¨
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

features, targets = origin_b_df.iloc[:, :-1], origin_b_df.iloc[:, -1]

X_train, X_test, y_train, y_test =\
train_test_split(features, targets, test_size=0.2, random_state=321)

dt_r = DecisionTreeRegressor(random_state=321)
rd_r = RandomForestRegressor(random_state=321)
gb_r = GradientBoostingRegressor(random_state=321)
xgb_r = XGBRegressor(random_state=321)
lgb_r = LGBMRegressor(random_state=321)

models = [dt_r, rd_r, gb_r, xgb_r, lgb_r]

for model in models:
    model.fit(X_train, y_train)
    prediction = model.predict(X_test)
    print(model.__class__.__name__)
    get_evaluation_negative(y_test, prediction)
```

<img width="245" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 42 39" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/115bf70e-b11d-41cd-b6b0-4705d561c11a">

```
# C02
ë²”ì£¼í˜• ë°ì´í„° ì¶”ê°€ ì‹œ ë¹„ì„ í˜• ë°ì´í„° ìˆ˜ì¹˜ê°€ ë†’ì•„ì§€ëŠ” ë¶€ë¶„ í™•ì¸
íŠ¸ë¦¬ê¸°ë°˜ì˜ ëª¨ë¸ì—ì„œ í›ˆë ¨ì´ ë” ì˜ë˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì—¬ íŠ¸ë¦¬ê¸°ë°˜ì˜ ëª¨ë¸ ì„ íƒ
```

<h2 id="cycle03">Cycle03</h2>
<p>ì´ìƒì¹˜ë¥¼ í™•ì¸í•˜ì˜€ê¸° ë•Œë¬¸ì— ì´ìƒì¹˜ ì œê±° í›„ í›ˆë ¨ ì§„í–‰</p>

```
# ì´ìƒì¹˜ ì œê±°ë¥¼ ìœ„í•œ í‘œì¤€í™” ì‘ì—…
from sklearn.preprocessing import StandardScaler

std = StandardScaler()
result = std.fit_transform(origin_b_df)
std_origin_b_df = pd.DataFrame(result, columns = origin_b_df.columns)
std_origin_b_df

# ì´ìƒì¹˜ í™•ì¸ ë° ì œê±°
condition = True
error_count = []

for column in std_origin_b_df.columns:
    if std_origin_b_df[column].between(-1.96, 1.96) is True:
        error_count.append(std_origin_b_df[column].between(-1.96, 1.96).count())
    condition &= std_origin_b_df[column].between(-1.96, 1.96)

std_origin_b_df = std_origin_b_df[condition]
std_origin_b_df

# ì´ìƒì¹˜ ì œê±°í•œ ë°ì´í„°ë¥¼ ì¸ë±ìŠ¤ ë²ˆí˜¸ì— ë§ê²Œ ê°€ì ¸ì˜¤ê¸°
origin_b_df = origin_b_df.iloc[std_origin_b_df.index].reset_index(drop=True)
origin_b_df
```

```
# ì„ í˜• íšŒê·€ ëª¨ë¸ í›ˆë ¨
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

features, targets = origin_b_df.iloc[:, :-1], origin_b_df.iloc[:, -1]

X_train, X_test, y_train, y_test = \
train_test_split(features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 141044.1497, RMSE: 375.5585, R2: 0.4673
```
<img width="244" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/32b007c5-2b8a-46b4-bcbd-1612e3a0c6f6">

```
# ë¹„ì„ í˜• ëª¨ë¸ í™•ì¸
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly_features = PolynomialFeatures(degree=2).fit_transform(features)

X_train, X_test, y_train, y_test =\
train_test_split(poly_features, targets, test_size=0.2, random_state=321)

l_r = LinearRegression()
l_r.fit(X_train, y_train)

prediction = l_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 125052.8676, RMSE: 353.6281, R2: 0.5277
```

<img width="240" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/74b3609d-fad1-4389-86d6-a806bd6ced3d">

```
# íŠ¸ë¦¬ ëª¨ë¸ í™•ì¸
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

features, targets = origin_b_df.iloc[:, :-1], origin_b_df.iloc[:, -1]

X_train, X_test, y_train, y_test =\
train_test_split(features, targets, test_size=0.2, random_state=321)

dt_r = DecisionTreeRegressor(random_state=321)
rd_r = RandomForestRegressor(random_state=321)
gb_r = GradientBoostingRegressor(random_state=321)
xgb_r = XGBRegressor(random_state=321)
lgb_r = LGBMRegressor(random_state=321)

models = [dt_r, rd_r, gb_r, xgb_r, lgb_r]

for model in models:
    model.fit(X_train, y_train)
    prediction = model.predict(X_test)
    print(model.__class__.__name__)
    get_evaluation_negative(y_test, prediction)

```
<img width="549" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/4dced628-6f61-4851-80a3-19516294b517">

<img width="672" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/17c5bdea-35a7-4665-b92e-1cc693132cc1">
<img width="384" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/62c2343a-c6b0-48e7-bc08-f40f41cb5e6b">


<h2 id="cycle04">Cycle04</h2>
<p>ê³¼ì í•©ì˜ ì •ë„ë¥¼ íŒë‹¨íˆê¸° ìœ„í•´ êµì°¨ê²€ì¦ì„ ì§„í–‰</p>
<p>ì´ì „ ê²€ì‚¬ì—ì„œ ë°ì´í„°ì˜ ì •ë„ë¥¼ ë´¤ì„ ë•Œ ê³¼ì í•©ì´ ìˆì„ ìˆ˜ ìˆë‹¤ëŠ” íŒë‹¨</p>

```
from sklearn.model_selection import cross_val_score, KFold

features, targets = origin_b_df.iloc[:,:-1], origin_b_df.iloc[:,-1]

kf = KFold(n_splits=10, random_state=124, shuffle=True)
scores = cross_val_score(lgb_r, features, targets , cv=kf)
scores
```

<img width="356" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/3c6cfa3f-85f0-43c3-a07c-70fc6460ad30">

```
# êµì°¨ê²€ì¦ ë° l2 ê·œì œ
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.pipeline import Pipeline


features, targets = origin_b_df.iloc[:,:-1], origin_b_df.iloc[:,-1]

X_train, X_test, y_train, y_test = \
train_test_split(features,targets, test_size=0.2, random_state=321)

kfold = KFold(n_splits=10, random_state=124, shuffle=True)

lgb_r = LGBMRegressor()

parameters = {
    'random_state': [321],
    'verbose': [-1]
}

g_lgb_r = GridSearchCV(lgb_r, param_grid=parameters, cv=kfold, scoring='neg_mean_squared_error')
g_lgb_r.fit(X_train, y_train)

# ìµœì ì˜ íŒŒë¼ë¯¸í„°ì™€ ì„±ëŠ¥ ì¶œë ¥
print("Best parameters:", g_lgb_r.best_params_)
print("Best cross-validation score: {:.3f}".format(-g_lgb_r.best_score_))

prediction = g_lgb_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

MSE: 76253.2006, RMSE: 276.1398, R2: 0.7120
```

<img width="669" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/2a1afd3e-d843-466d-8add-09a14d6d03d0">
<img width="377" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/4c4f900e-f56b-4c8e-a7ea-5a31808a314e">




<h2 id='cycle05'>Cycle05</h2>
<p>ëª¨ë¸ì˜ ì¼ë°˜í™”ë¥¼ ìœ„í•´ ë‹¤ì¤‘ê³µì„ ì„±ê³¼ ìƒê´€ê´€ê³„í™•ì¸ í›„ ë¶ˆí•„ìš” featrue ì œê±°.</p>

```
origin_b_df = origin_b_df.drop(labels = ['Holiday'], axis = 1)
origin_b_df = origin_b_df.drop(labels = ['Humidity(%)', 'Wind speed (m/s)', 'Solar Radiation (MJ/m2)'], axis = 1)
```

<img width="198" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 51 00" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/5449dac4-43f4-4516-8100-e8e33278d2c8">
<img width="477" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 51 25" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/d1dd7174-b767-4c9d-8b15-0065e57ca0e0">


```
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.pipeline import Pipeline


features, targets = origin_b_df.iloc[:,:-1], origin_b_df.iloc[:,-1]

X_train, X_test, y_train, y_test = \
train_test_split(features,targets, test_size=0.2, random_state=321)

kfold = KFold(n_splits=10, random_state=124, shuffle=True)

lgb_r = LGBMRegressor()

parameters = {
    'random_state': [321],
    # 'reg_lambda': [100],
    'verbose': [-1]
}

g_lgb_r = GridSearchCV(lgb_r, param_grid=parameters, cv=kfold, scoring='neg_mean_squared_error')
g_lgb_r.fit(X_train, y_train)

# ìµœì ì˜ íŒŒë¼ë¯¸í„°ì™€ ì„±ëŠ¥ ì¶œë ¥
print("Best parameters:", g_lgb_r.best_params_)
print("Best cross-validation score: {:.3f}".format(-g_lgb_r.best_score_))

prediction = g_lgb_r.predict(X_test)
get_evaluation_negative(y_test, prediction)


MSE: 73783.6514, RMSE: 271.6315, R2: 0.7213
```

<img width="677" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/fd40add0-21dd-4545-b6bc-c9930a04798d">
<img width="391" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/6a045345-f947-47aa-8e35-7c7680b61d83">



<h2 id='cycle06'>Cycle06</h2>
<p>ê° ìˆ˜ì¹˜í˜• ë°ì´í„°ì— ëŒ€í•˜ì—¬ powertransform ì„ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜í™” ê°•í™”</p>
<p>Ridge ê·œì œë¥¼ í†µí•´ ê³¼ì í•¨ì„ ì¶”ê°€ì ìœ¼ë¡œ ë°©ì§€</p>

```
from sklearn.preprocessing import PowerTransformer

columns = pre_b_df.iloc[:, :-1].columns
power_b_df = pre_b_df.copy()

for column in columns:
    ptf = PowerTransformer(standardize=False)
    result = ptf.fit_transform(pre_b_df[[column]])
    power_b_df[column] = result

power_b_df


- ë°ì´í„° ì „ì²˜ë¦¬
power_b_df = power_b_df.drop(labels = ['Wind speed (m/s)'], axis = 1)

from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.pipeline import Pipeline


features, targets = power_b_df.iloc[:,:-1], power_b_df.iloc[:,-1]

X_train, X_test, y_train, y_test = \
train_test_split(features,targets, test_size=0.2, random_state=321)

kfold = KFold(n_splits=10, random_state=124, shuffle=True)

lgb_r = LGBMRegressor()

parameters = {
    'random_state': [321],
    'reg_lambda': [100],
    'verbose': [-1]
}

g_lgb_r = GridSearchCV(lgb_r, param_grid=parameters, cv=kfold, scoring='neg_mean_squared_error')
g_lgb_r.fit(X_train, y_train)

# ìµœì ì˜ íŒŒë¼ë¯¸í„°ì™€ ì„±ëŠ¥ ì¶œë ¥
print("Best parameters:", g_lgb_r.best_params_)
print("Best cross-validation score: {:.3f}".format(-g_lgb_r.best_score_))

prediction = g_lgb_r.predict(X_test)
get_evaluation_negative(y_test, prediction)


MSE: 88352.2831, RMSE: 297.2411, R2: 0.7834
```

<img width="668" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 54 16" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/68c45d1c-bdb7-4b96-addd-f8636b4549f1">
<img width="411" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/cc6f7c90-9e70-4001-a682-463e180d117a">


<h2 id='cycle07'>Cycle07</h2>
<p>ìˆ˜ì¹˜í­ì„ ë” ì¢í ìˆ˜ ìˆëŠ” ì§€ ì¶”ê°€ ì „ì²˜ë¦¬ ì§„í–‰</p>
<p>ë¶ˆí•„ìš” feature ì‚­ì œ.</p>

```
    power_b_df = power_b_df.drop(labels = ['Snowfall (cm)'], axis=1)

    from lightgbm import LGBMRegressor
    from sklearn.model_selection import train_test_split, GridSearchCV, KFold
    from sklearn.pipeline import Pipeline


    features, targets = power_b_df.iloc[:,:-1], power_b_df.iloc[:,-1]

    X_train, X_test, y_train, y_test = \
    train_test_split(features,targets, test_size=0.2, random_state=321)

    kfold = KFold(n_splits=10, random_state=124, shuffle=True)

    lgb_r = LGBMRegressor()

    parameters = {
        'random_state': [321],
        'reg_lambda': [100],
        'verbose': [-1]
    }

    g_lgb_r = GridSearchCV(lgb_r, param_grid=parameters, cv=kfold, scoring='neg_mean_squared_error')
    g_lgb_r.fit(X_train, y_train)

    # ìµœì ì˜ íŒŒë¼ë¯¸í„°ì™€ ì„±ëŠ¥ ì¶œë ¥
    print("Best parameters:", g_lgb_r.best_params_)
    print("Best cross-validation score: {:.3f}".format(-g_lgb_r.best_score_))

    prediction = g_lgb_r.predict(X_test)
    get_evaluation_negative(y_test, prediction)

    MSE: 88007.3417, RMSE: 296.6603, R2: 0.7842
```
<img width="191" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 56 11" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/1d4ec3f0-4180-487a-9b19-7b78e66669f3">
<img width="500" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-15 á„‹á…©á„’á…® 10 56 27" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/152390bd-2e9b-4934-92b4-1f31707f47f7">

```
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

X_train, X_test, y_train, y_test =\
train_test_split(features, targets, test_size=0.2, random_state=321)

r_X_train, v_X_train, r_y_train, v_y_train = \
train_test_split(X_train, y_train, test_size= 0.3, random_state=321)

# ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡

g_lgb_r.fit(r_X_train, r_y_train)
prediction_r_train = g_lgb_r.predict(r_X_train)
prediction_v_train = g_lgb_r.predict(v_X_train)

# í‰ê°€

get_evaluation_negative(r_y_train, prediction_r_train)
get_evaluation_negative(v_y_train, prediction_v_train)

# ì‹œê°í™”

fig, ax = plt.subplots(1, 2, figsize=(12, 5))
ax[0].scatter(r_y_train, prediction_r_train, edgecolors='red', c='red', alpha=0.2)
ax[0].plot([r_y_train.min(), r_y_train.max()], [r_y_train.min(), r_y_train.max()], 'k--')
ax[0].set_title('Train Data Prediction')

ax[1].scatter(v_y_train, prediction_v_train, edgecolors='red', c='blue', alpha=0.2)
ax[1].plot([v_y_train.min(), v_y_train.max()], [v_y_train.min(), v_y_train.max()], 'k--')
ax[1].set_title('Validation Data Prediction')
plt.show()

MSE: 69007.0353, RMSE: 262.6919, R2: 0.8351
MSE: 99120.6772, RMSE: 314.8344, R2: 0.7621

import matplotlib.pyplot as plt

prediction = g_lgb_r.predict(X_test)
get_evaluation_negative(y_test, prediction)

fig, ax = plt.subplots()
ax.scatter(y_test, prediction, edgecolors='red', c='orange', alpha=0.2)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')
plt.show()
```


<img width="664" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/8a4f0d24-c7f6-49fa-92bd-6a9236670b8f">
<img width="380" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/abc9c46a-f193-4d42-8b32-ff07234be844">

<img width="735" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/ad167a09-729a-4da8-8d51-d4efaf730d50">
<img width="730" alt="image" src="https://github.com/kimgusan/Machine_Learning/assets/156397911/f4b05b04-870f-4e6c-bff0-6f2b98515aff">

<hr>

-   ì •ë¦¬

    - ê³¼ì í•© ë¬¸ì œ ì‹ë³„: Validation ê·¸ë˜í”„ë¥¼ í†µí•´ Train ë°ì´í„°ì™€ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì— ê³¼ì í•©ì´ ìˆìŒì„ íŒë‹¨í–ˆìŠµë‹ˆë‹¤.
    - ëª¨ë¸ ì¼ë°˜í™” ê°œì„ : ë‹¤ì¤‘ê³µì„ ì„± ê²€ì‚¬, Power Transform ì ìš©, ìƒê´€ê´€ê³„ ë¶„ì„ì„ í†µí•´ ëª¨ë¸ì˜ ì¼ë°˜í™”ë¥¼ ë„ëª¨í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶„ì„ì„ í†µí•´ ëª¨ë¸ì´ ë”ìš± ê°•ê±´í•´ì§ˆ ìˆ˜ ìˆë„ë¡ ì¡°ì¹˜ë¥¼ ì·¨í–ˆìŠµë‹ˆë‹¤.
    - L2 ê·œì œ ì ìš©: êµì°¨ ê²€ì¦ì„ í†µí•´ L2 ê·œì œë¥¼ ì ìš©í•¨ìœ¼ë¡œì¨ ê³¼ì í•©ì„ ë°©ì§€í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê²€ì¦ ë°ì´í„°ì—ì„œì˜ ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.

-   ê²°ë¡ 
  
    - ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ìì „ê±° ëŒ€ì—¬ ìˆ˜ëŸ‰ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì£¼ìš” ìš”ì†Œë“¤ì„ íŒŒì•…í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íš¨ê³¼ì ì¸ ìì „ê±° ë°°ì¹˜ì— ëŒ€í•˜ì—¬ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ìˆ˜ë¦½í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.
    - ëª¨ë¸ì˜ ê³¼ì í•© ë¬¸ì œë¥¼ ì‹ë³„í•˜ê³  ì´ë¥¼ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì ìš©í•¨ìœ¼ë¡œì¨, ëª¨ë¸ì˜ ì‹ ë¢°ì„±ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.
    - ì´ ê²°ê³¼ëŠ” ìì „ê±° ê³µìœ  íšŒì‚¬ì—ê²Œ ìì „ê±° ë°°ì¹˜ ê³„íšì„ ë” ì˜ ìˆ˜ë¦½í•˜ê³ , ê³ ê°ì˜ ë¶ˆí¸ì„±ì„ í•´ì†Œí•˜ë©° íšŒì› ìœ ì¹˜ì— ë„ì›€ì´ ë  ê²ƒìœ¼ë¡œ ì‚¬ë£Œë©ë‹ˆë‹¤.
