{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2198d41a-0e21-4455-8bb9-9267894ba972",
   "metadata": {},
   "source": [
    "### 서포트 벡터 머신 (SVM, Support Vector Machine)\n",
    "- 기존의 분류 방법들은 '오류율 최소화'의 목적으로 설계되었다면, SVM은 두 부류 사이에 존재하는 '여백 최대화'의 목적으로 설계되었다.\n",
    "- 분류 문제를 해결하는 지도 학습 모델 중 하나이며, 결정 경계라는 데이터 간 경계를 정의함으로써 분류를 할 수 있다.\n",
    "- 새로운 데이터가 경계를 기준으로 어떤 방향에 잡히는지를 확인함으로써 해당 데이터의 카테고리를 예측할 수 있다.\n",
    "- 데이터가 어느 카테고리에 속할 지 판단하기 위해 가장 적절한 경계인 결정 경계를 찾는 선형 모델이다.\n",
    "\n",
    "<img src='./images/support_vector_machine01.png' width='400px' style='margin-bottom: 60px'>\n",
    "\n",
    "#### 서포트 벡터 (Support Vector)\n",
    "- 결정 경계를 결정하는 데이터(벡터)들을 서포트 벡터라고 부른다.\n",
    "- 벡터들이 결정 경계 (Decision boundary)를 결정한다.\n",
    "- 서포트 벡터와 결정 경계간의 거리를 마진(Margin)이라고 부르고, 마진이 크면 클 수록 좋은 결정 경계가 된다.\n",
    "- 서포트 벡터들을 통해 결정 경계를 결정하게 되고, 다른 학습 데이터들은 무시될 수 있기 때문에 SVM의 속도가 빠를 수 있다.\n",
    "\n",
    "### 결정 경계 (Decision boundary)\n",
    "- 새로운 데이터가 들어오더라도 결정 경계를 중심으로 두 집단이 멀리 떨어져 있어야 두 집단을 잘 구분할 수 있기 때문에 일반화하기 쉬워진다.\n",
    "- 독립 변수의 차원보다 한차원 낮아지며, N차원 공간에서 한 차원 낮은 N-1차원의 결정 경계가 생긴다.\n",
    "  즉, 2차원 공간에서는 결정 경계는 선으로 결정되고,  \n",
    "  고차원에서는 결정 경계는 선이 아닌 평면 이상의 도형이며, 이를 \"초평면(Hyperplane)\"이라고 부른다.\n",
    "\n",
    "<img src='./images/support_vector_machine02.png' width='400px' style='margin-bottom: 60px'>\n",
    "\n",
    "#### 하드 마진(Hard margin)\n",
    "- 매우 엄격하게 집단을 구분하는 방법으로 이상치를 허용해주지 않는 방법이다.\n",
    "- 이상치를 허용하지 않기 때문에 과적합이 발생하기 쉽고, 최적의 결정경계를 잘못 구분하거나 못찾는 경우가 생길 수 있다.\n",
    "- C(cost)는 패널티를 조절할 수 있고, 값이 커질수록 결정 경계가 데이터에 더 정확하게 맞춰진다\n",
    "- C를 낮추면 일을 덜 하게 하는 것이고, C를 높이면 일을 더 해서 섬세하게 찾아낸다.\n",
    "- C가 너무 낮으면 nuderfitting될 가능성이 커지고, C가 너무 높으면 overfitting이 발생할 수 있다.\n",
    "\n",
    "<img src='./images/hard_margin.png' width='350px' style='margin-bottom: 60px'>  \n",
    "  \n",
    "#### 소프트 마진(Soft margin)\n",
    "- 이상치를 허용해서 일부 데이터를 잘못 분류하더라도 나머지 데이터를 더욱 잘 분류해주는 방법이다.\n",
    "- 이상치 허용으로 인해 데이터의 패턴을 잘 감지 못하는 문제점이 생길 수 있다.\n",
    "  \n",
    "<img src='./images/soft_margin.png' width='550px' style='margin-bottom: 60px'>  \n",
    "\n",
    "> 🎆 정리  \n",
    "> 서포트 벡터 머신 알고리즘을 적용한 SVC 모델의 하이퍼파라미터은 Regularization cost, C에 값을 전달하여 ξ(패널티)를 조절할 수 있다.\n",
    "C가 클수록 loss function에서 오차항인 ξ<sub>i</sub>의 영향력이 커지게 되기 때문에 마진의 크기가 줄어들고(하드 마진), 반대로 C가 작을 수록 마진의 크기가 늘어난다(소프트 마진). 적절히 조절하면 성능이 좋아질 수있다.\n",
    "\n",
    "**손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bff1e9-6339-46e0-8078-3eb18621a7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b72a21-50b8-4b7d-bbaf-63d48d5d8171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901146ac-e8ec-4006-adf2-b74a4a6d33f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef274ceb-cafb-4ca5-be28-45d1cf1228aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763c9c2-8228-4346-9eb0-c00996de159d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70915264-b484-4691-9671-5430e1f790b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
